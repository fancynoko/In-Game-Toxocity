{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wX8murT95BCq"
      },
      "source": [
        "#1.Load the data from Github"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SDsyVR58RIAC"
      },
      "outputs": [],
      "source": [
        "#Import Libraries \n",
        "import datetime\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.metrics import classification_report, f1_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tOGpYWOrsnXb"
      },
      "outputs": [],
      "source": [
        "#Google Drive Access Setup\n",
        "!pip install -U -q PyDrive\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "# Authenticate and create the PyDrive client.\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "mb-PJpDxJCnB",
        "outputId": "a159b4a7-fd9e-46cc-b7e2-40c875dec96f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------------------------------------\n",
            "Size of training dataset: 26078\n",
            "Size of validation dataset: 8705\n",
            "Size of testing dataset: 500\n",
            "------------------------------------\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-b987f488-ebab-417f-86f4-ad51617ff859\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sents</th>\n",
              "      <th>labels</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>wow</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>WTF</td>\n",
              "      <td>T</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>wpe wpe</td>\n",
              "      <td>O O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>hahaha</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>wtf</td>\n",
              "      <td>T</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>i cant [SEPA] play [SEPA] with 4 trash</td>\n",
              "      <td>P O SEPA O SEPA O O O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>bg</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>#ERROR!</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>gg [SEPA] report my team rat [SEPA] please</td>\n",
              "      <td>S SEPA S P O S SEPA O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>ez mid</td>\n",
              "      <td>S S</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>hahah</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>my arrows [SEPA] always decent [SEPA] fuck u</td>\n",
              "      <td>P O SEPA O O SEPA T P</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>gh</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>always engage at bot lc cant takle bot then top</td>\n",
              "      <td>O O O S D O O S O S</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>cmon</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>the comeback is real :)</td>\n",
              "      <td>O O O O O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>him mid vs me</td>\n",
              "      <td>P S O P</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>just end [SEPA] i wan nex game</td>\n",
              "      <td>O O SEPA P O O O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>g</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>he is not losing</td>\n",
              "      <td>P O O O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>Pls report sb thanks</td>\n",
              "      <td>O S C O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>omg</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>ggwp</td>\n",
              "      <td>S</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>WP cap</td>\n",
              "      <td>S O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>lo [SEPA] lol</td>\n",
              "      <td>O SEPA O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>fuyckjerfe</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>gg noob mid invoker</td>\n",
              "      <td>S T S C</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>i mean not everyone on ur team is dumb enough ...</td>\n",
              "      <td>P O O P O P O O T P O D O O O O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>8</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>it just random</td>\n",
              "      <td>P O O</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-b987f488-ebab-417f-86f4-ad51617ff859')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-b987f488-ebab-417f-86f4-ad51617ff859 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-b987f488-ebab-417f-86f4-ad51617ff859');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "                                                sents  \\\n",
              "0                                                 wow   \n",
              "1                                                 WTF   \n",
              "2                                             wpe wpe   \n",
              "3                                              hahaha   \n",
              "4                                                 wtf   \n",
              "5              i cant [SEPA] play [SEPA] with 4 trash   \n",
              "6                                                  bg   \n",
              "7                                             #ERROR!   \n",
              "8          gg [SEPA] report my team rat [SEPA] please   \n",
              "9                                              ez mid   \n",
              "10                                              hahah   \n",
              "11       my arrows [SEPA] always decent [SEPA] fuck u   \n",
              "12                                                 gh   \n",
              "13    always engage at bot lc cant takle bot then top   \n",
              "14                                               cmon   \n",
              "15                            the comeback is real :)   \n",
              "16                                      him mid vs me   \n",
              "17                     just end [SEPA] i wan nex game   \n",
              "18                                                  g   \n",
              "19                                   he is not losing   \n",
              "20                               Pls report sb thanks   \n",
              "21                                                omg   \n",
              "22                                               ggwp   \n",
              "23                                             WP cap   \n",
              "24                                      lo [SEPA] lol   \n",
              "25                                         fuyckjerfe   \n",
              "26                                gg noob mid invoker   \n",
              "27  i mean not everyone on ur team is dumb enough ...   \n",
              "28                                                  8   \n",
              "29                                     it just random   \n",
              "\n",
              "                             labels  \n",
              "0                                 O  \n",
              "1                                 T  \n",
              "2                               O O  \n",
              "3                                 O  \n",
              "4                                 T  \n",
              "5             P O SEPA O SEPA O O O  \n",
              "6                                 O  \n",
              "7                                 O  \n",
              "8             S SEPA S P O S SEPA O  \n",
              "9                               S S  \n",
              "10                                O  \n",
              "11            P O SEPA O O SEPA T P  \n",
              "12                                O  \n",
              "13              O O O S D O O S O S  \n",
              "14                                O  \n",
              "15                        O O O O O  \n",
              "16                          P S O P  \n",
              "17                 O O SEPA P O O O  \n",
              "18                                O  \n",
              "19                          P O O O  \n",
              "20                          O S C O  \n",
              "21                                O  \n",
              "22                                S  \n",
              "23                              S O  \n",
              "24                         O SEPA O  \n",
              "25                                O  \n",
              "26                          S T S C  \n",
              "27  P O O P O P O O T P O D O O O O  \n",
              "28                                O  \n",
              "29                            P O O  "
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import pandas as pd \n",
        "\n",
        "# test_input = 'file:///content/drive/MyDrive/test_without_labels.csv'\n",
        "# train_input = 'file:///content/drive/MyDrive/train.csv'\n",
        "# valid_input = 'file:///content/drive/MyDrive/val.csv'\n",
        "\n",
        "# id = '15ytwyux4X_hfeo9rKc54mScR29UOG6BL' \n",
        "# downloaded = drive.CreateFile({'id':id}) \n",
        "# downloaded.GetContentFile('sample.csv')\n",
        "\n",
        "id = '186aUyJjlJx8tOKG1_Eli41_fQ4C9wDCi'\n",
        "downloaded = drive.CreateFile({'id':id}) \n",
        "downloaded.GetContentFile('train.csv')\n",
        "\n",
        "id = '1Gns3_mTVRSCKCvzTvCWrlI8HJ0XMyBHD'\n",
        "downloaded = drive.CreateFile({'id':id}) \n",
        "downloaded.GetContentFile('val.csv')\n",
        "\n",
        "id = '11L-S5S3cLFE7pM3RVohFS9P5XX9QzMqe'\n",
        "downloaded = drive.CreateFile({'id':id}) \n",
        "downloaded.GetContentFile('test_without_labels.csv')\n",
        "\n",
        "train = pd.read_csv('/content/train.csv')\n",
        "val = pd.read_csv('/content/val.csv')\n",
        "test = pd.read_csv('/content/test_without_labels.csv')\n",
        "\n",
        "print(\"------------------------------------\")\n",
        "print(\"Size of training dataset: {0}\".format(len(train)))\n",
        "print(\"Size of validation dataset: {0}\".format(len(val)))\n",
        "print(\"Size of testing dataset: {0}\".format(len(test)))\n",
        "print(\"------------------------------------\")\n",
        "train.head(30)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N7cK7PhPE-ds"
      },
      "outputs": [],
      "source": [
        "val.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JFM2u21AFAPw"
      },
      "outputs": [],
      "source": [
        "test.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "viApnwHa3noW"
      },
      "outputs": [],
      "source": [
        "# Extract the sents and labels and store into List\n",
        "\n",
        "# Get the list of training data (sents)\n",
        "train_sents = list(train['sents'])\n",
        "# Get the list of corresponding labels for the training data (sents)\n",
        "train_labels = list(train['labels'])\n",
        "\n",
        "\n",
        "# Get the list of validation data (sents)\n",
        "val_sents = list(val['sents'])\n",
        "# Get the list of corresponding labels for the validation data (sents)\n",
        "val_labels = list(val['labels'])\n",
        "\n",
        "\n",
        "# Get the list of testing data (sents)\n",
        "test_sents = list(test['sents'])\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9cNiOjSo4CFS"
      },
      "outputs": [],
      "source": [
        "train_sents[:10]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OPn965FlDGyZ"
      },
      "source": [
        "# 2.Input Embedding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5iDyhK8uRohh"
      },
      "source": [
        "###Word Embedding "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vwgM5NLNRtW-",
        "outputId": "1d16894d-b0e7-4122-f64f-722921a20667"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[==================================================] 100.0% 104.8/104.8MB downloaded\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "(11243, 25)"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#Using pre-trainded embedding \n",
        "import numpy as np \n",
        "import gensim.downloader as api\n",
        "word_emb_model = api.load(\"glove-twitter-25\") \n",
        "\n",
        "# Embedding dimension\n",
        "EMBEDDING_DIM = 25\n",
        "\n",
        "word_to_ix = {}\n",
        "for sentence in train_sents + val_sents + test_sents:\n",
        "    for word in sentence.split():\n",
        "        word = word.lower()\n",
        "        if word not in word_to_ix:\n",
        "            word_to_ix[word] = len(word_to_ix)\n",
        "word_list = list(word_to_ix.keys())\n",
        "ix_to_word = {v:k for k,v in word_to_ix.items()}\n",
        "\n",
        "# How many unique words\n",
        "VOCAB_SIZE = len(word_to_ix)\n",
        "\n",
        "# Addtional Words\n",
        "START_TAG = \"<START>\"\n",
        "STOP_TAG = \"<STOP>\"\n",
        "SEPA = \"sepa\"\n",
        "tag_to_ix = {START_TAG:0, STOP_TAG:1, SEPA:3}\n",
        "for tags in train_labels+val_labels:\n",
        "    for tag in tags:\n",
        "      tag = tag.lower()\n",
        "      if tag not in tag_to_ix:\n",
        "        tag_to_ix[tag] = len(tag_to_ix)\n",
        "\n",
        "# Create a Matrix: word -> vector\n",
        "embedding_matrix = []\n",
        "for word in word_list:\n",
        "    try:\n",
        "        embedding_matrix.append(word_emb_model.self[word])\n",
        "    except:\n",
        "        embedding_matrix.append([0]*EMBEDDING_DIM)\n",
        "embedding_matrix = np.array(embedding_matrix)\n",
        "embedding_matrix.shape\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zPH6HUMoteD8",
        "outputId": "c7e28952-fb5e-4bcb-ce3c-a34e3a088322"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "26078\n"
          ]
        }
      ],
      "source": [
        "def to_index(data, to_ix, lower=True):\n",
        "    input = []\n",
        "    for sent in data:\n",
        "        input.append([to_ix[w.lower() if lower else w] for w in sent.split()])\n",
        "    return input\n",
        "train_input_index =  to_index(train_sents,word_to_ix)\n",
        "train_output_index = to_index(train_labels,tag_to_ix)\n",
        "val_input_index = to_index(val_sents,word_to_ix)\n",
        "val_output_index = to_index(val_labels,tag_to_ix)\n",
        "test_input_index = to_index(test_sents,word_to_ix)\n",
        "\n",
        "# print('Input: ', len(train_input_index[1]))\n",
        "# print('Output: ', len(train_output_index[1]))\n",
        "print(len(train_input_index))\n",
        "# print(len(train_output_index[5]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-vvT1E9ln3fo"
      },
      "source": [
        "###Pos Tagging & Dependency Path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R6UqYRern6Hi"
      },
      "outputs": [],
      "source": [
        "import spacy\n",
        "# Load your usual SpaCy model (one of SpaCy English models)\n",
        "nlp = spacy.load('en_core_web_sm')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZtAwZSAaoMK0"
      },
      "outputs": [],
      "source": [
        "#Generate the part of speech tag for the training data\n",
        "\n",
        "def getpos_dep(sents):\n",
        "  p = []\n",
        "  d = []\n",
        "  for s in sents:\n",
        "    doc = nlp(s)\n",
        "    p_temp = []\n",
        "    d_temp = []\n",
        "    for token in doc:\n",
        "      p_temp.append(token.tag_)\n",
        "      d_temp.append(token.dep_)\n",
        "    p.append(p_temp)\n",
        "    d.append(d_temp)\n",
        "  return p,d"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sxlpkj0EoRs4"
      },
      "outputs": [],
      "source": [
        "#Convert the pos tagging, dependancy tagging into index\n",
        "pos_idx = {}\n",
        "dep_idx = {}\n",
        "\n",
        "train_pos, train_dep = getpos_dep(train_sents)\n",
        "val_pos, valA_dep = getpos_dep(val_sents)\n",
        "test_pos, test_dep = getpos_dep(test_sents)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "21d4izIT8u78"
      },
      "outputs": [],
      "source": [
        "for ps in train_pos + val_pos + test_pos:\n",
        "  for p in ps:\n",
        "    if p not in pos_idx:\n",
        "      pos_idx[p] = len(pos_idx)\n",
        "\n",
        "for des in train_dep + valA_dep + test_dep:\n",
        "  for de in des:\n",
        "    if de not in dep_idx:\n",
        "      dep_idx[de] = len(dep_idx)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jVup7bg9NJRi",
        "outputId": "3addaea3-64d9-4f1f-9f62-0bffe75c4910"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "PoS Embedding Matrix: (11243, 49)\n",
            "[[0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " ...\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]]\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "from nltk import pos_tag\n",
        "\n",
        "# Total number of PoS tags\n",
        "POSTAG_SIZE = len(list(pos_idx.keys()))\n",
        "postag_list = list(pos_idx.keys())\n",
        "\n",
        "# PoS tag <--> word index\n",
        "pos_to_ix = { k:v for v, k in enumerate(postag_list) }\n",
        "\n",
        "# PoS Embedding Matrix\n",
        "pos_embedding_matrix = np.zeros((VOCAB_SIZE, POSTAG_SIZE))\n",
        "for word_idx, word in enumerate(word_list):\n",
        "    tag = pos_tag([word])\n",
        "    pos_idx = pos_to_ix[tag[0][1]]\n",
        "    pos_embedding_matrix[word_idx, pos_idx] = 1\n",
        "\n",
        "print(f'PoS Embedding Matrix: {pos_embedding_matrix.shape}\\n{pos_embedding_matrix}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9b2wFX37fm3j"
      },
      "source": [
        "# 3.Slot Filling/Tagging Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CsZ_JjBzSb9K"
      },
      "source": [
        "####CRF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uCwmBwezSfAL"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.autograd as autograd\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eiLBOqpuSi2C"
      },
      "outputs": [],
      "source": [
        "#Reference from Lab code\n",
        "class CRF_layer(torch.nn.Module):\n",
        "    '''\n",
        "    Claim: The CRF_layer Code is derived from PyTorch Tutorial. We wrote a wrapper class for it. \n",
        "    More: https://pytorch.org/tutorials/beginner/nlp/advanced_tutorial.html\n",
        "    '''\n",
        "    def __init__(self, tag_to_ix):\n",
        "        \"\"\"\n",
        "        \"\"\"\n",
        "        super(CRF_layer, self).__init__()\n",
        "\n",
        "        self.vocab_size = VOCAB_SIZE\n",
        "        self.tag_to_ix = tag_to_ix\n",
        "        self.tagset_size = len(tag_to_ix)\n",
        "\n",
        "        # Matrix of transition parameters.  Entry i,j is the score of\n",
        "        # transitioning *to* i *from* j.\n",
        "        self.transitions = nn.Parameter(\n",
        "            torch.randn(self.tagset_size, self.tagset_size))\n",
        "\n",
        "        # These two statements enforce the constraint that we never transfer\n",
        "        # to the start tag and we never transfer from the stop tag\n",
        "        self.transitions.data[tag_to_ix[START_TAG], :] = -10000\n",
        "        self.transitions.data[:, tag_to_ix[STOP_TAG]] = -10000\n",
        "        \n",
        "    def forward(self, feats):\n",
        "        # Do the forward algorithm to compute the partition function\n",
        "        init_alphas = torch.full((1, self.tagset_size), -10000.).to(device)\n",
        "        # START_TAG has all of the score.\n",
        "        init_alphas[0][self.tag_to_ix[START_TAG]] = 0.\n",
        "\n",
        "        # Wrap in a variable so that we will get automatic backprop\n",
        "        forward_var = init_alphas\n",
        "\n",
        "        # Iterate through the sentence\n",
        "        for feat in feats:\n",
        "            alphas_t = []  # The forward tensors at this timestep\n",
        "            for next_tag in range(self.tagset_size):\n",
        "                # broadcast the emission score: it is the same regardless of\n",
        "                # the previous tag\n",
        "                emit_score = feat[next_tag].view(\n",
        "                    1, -1).expand(1, self.tagset_size)\n",
        "                # the ith entry of trans_score is the score of transitioning to\n",
        "                # next_tag from i\n",
        "                trans_score = self.transitions[next_tag].view(1, -1)\n",
        "                # The ith entry of next_tag_var is the value for the\n",
        "                # edge (i -> next_tag) before we do log-sum-exp\n",
        "                next_tag_var = forward_var + trans_score + emit_score\n",
        "                # The forward variable for this tag is log-sum-exp of all the\n",
        "                # scores.\n",
        "                alphas_t.append(self._log_sum_exp(next_tag_var).view(1))\n",
        "            forward_var = torch.cat(alphas_t).view(1, -1)\n",
        "        terminal_var = forward_var + self.transitions[self.tag_to_ix[STOP_TAG]]\n",
        "        alpha = self._log_sum_exp(terminal_var)\n",
        "        return alpha\n",
        "\n",
        "    def decode(self, feats):\n",
        "        backpointers = []\n",
        "\n",
        "        # Initialize the viterbi variables in log space\n",
        "        init_vvars = torch.full((1, self.tagset_size), -10000.).to(device)\n",
        "        init_vvars[0][self.tag_to_ix[START_TAG]] = 0\n",
        "\n",
        "        # forward_var at step i holds the viterbi variables for step i-1\n",
        "        forward_var = init_vvars\n",
        "        for feat in feats:\n",
        "            bptrs_t = []  # holds the backpointers for this step\n",
        "            viterbivars_t = []  # holds the viterbi variables for this step\n",
        "\n",
        "            for next_tag in range(self.tagset_size):\n",
        "                # next_tag_var[i] holds the viterbi variable for tag i at the\n",
        "                # previous step, plus the score of transitioning\n",
        "                # from tag i to next_tag.\n",
        "                # We don't include the emission scores here because the max\n",
        "                # does not depend on them (we add them in below)\n",
        "                next_tag_var = forward_var + self.transitions[next_tag]\n",
        "                best_tag_id = self._argmax(next_tag_var)\n",
        "                bptrs_t.append(best_tag_id)\n",
        "                viterbivars_t.append(next_tag_var[0][best_tag_id].view(1))\n",
        "            # Now add in the emission scores, and assign forward_var to the set\n",
        "            # of viterbi variables we just computed\n",
        "            forward_var = (torch.cat(viterbivars_t) + feat).view(1, -1)\n",
        "            backpointers.append(bptrs_t)\n",
        "\n",
        "        # Transition to STOP_TAG\n",
        "        terminal_var = forward_var + self.transitions[self.tag_to_ix[STOP_TAG]]\n",
        "        best_tag_id = self._argmax(terminal_var)\n",
        "        path_score = terminal_var[0][best_tag_id]\n",
        "\n",
        "        # Follow the back pointers to decode the best path.\n",
        "        best_path = [best_tag_id]\n",
        "        for bptrs_t in reversed(backpointers):\n",
        "            best_tag_id = bptrs_t[best_tag_id]\n",
        "            best_path.append(best_tag_id)\n",
        "        # Pop off the start tag (we dont want to return that to the caller)\n",
        "        start = best_path.pop()\n",
        "        assert start == self.tag_to_ix[START_TAG]  # Sanity check\n",
        "        best_path.reverse()\n",
        "        return path_score, best_path\n",
        "        \n",
        "    def _score_sentence(self, feats, tags):\n",
        "        # Gives the score of a provided tag sequence\n",
        "        score = torch.zeros(1).to(device)\n",
        "        tags = torch.cat([torch.tensor([self.tag_to_ix[START_TAG]], dtype=torch.long).to(device), tags])\n",
        "        for i, feat in enumerate(feats):\n",
        "            score = score + \\\n",
        "                self.transitions[tags[i + 1], tags[i]] + feat[tags[i + 1]]\n",
        "        score = score + self.transitions[self.tag_to_ix[STOP_TAG], tags[-1]]\n",
        "        return score\n",
        "\n",
        "    def _argmax(self, vec):\n",
        "        # return the argmax as a python int\n",
        "        _, idx = torch.max(vec, 1)\n",
        "        return idx.item()\n",
        "\n",
        "    # Compute log sum exp in a numerically stable way for the forward algorithm\n",
        "    def _log_sum_exp(self, vec):\n",
        "        max_score = vec[0, self._argmax(vec)]\n",
        "        max_score_broadcast = max_score.view(1, -1).expand(1, vec.size()[1])\n",
        "        return max_score + \\\n",
        "            torch.log(torch.sum(torch.exp(vec - max_score_broadcast)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vcMBGrP0SxlP"
      },
      "source": [
        "###Attention"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5qczEEeTSwtm"
      },
      "outputs": [],
      "source": [
        "#Create the attention layer \n",
        "class Attention_layer(torch.nn.Module):\n",
        "    def __init__(self, input_size, attention_type='dot', dropout_rate=0.5):\n",
        "        \"\"\"\n",
        "        Define the trainable parameters for attention calculation.\n",
        "        Inputs:\n",
        "            inputsize:          input dimension size \n",
        "            attention_type:     dot | scaled_dot | cos_sim\n",
        "        \"\"\"\n",
        "        super(Attention_layer, self).__init__()\n",
        "        self.attention_type = attention_type\n",
        "\n",
        "        self.K = nn.Linear(input_size, input_size, bias=False)\n",
        "        self.Q = nn.Linear(input_size, input_size, bias=False)\n",
        "        self.V = nn.Linear(input_size, input_size, bias=False)\n",
        "\n",
        "        self.K_dropout = nn.Dropout(p=dropout_rate)\n",
        "        self.Q_dropout = nn.Dropout(p=dropout_rate)\n",
        "        self.V_dropout = nn.Dropout(p=dropout_rate)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Attention calculation\n",
        "        For self-attention, the Key, Query and Value all equal to x\n",
        "        \"\"\"\n",
        "        out_key = self.K_dropout(self.K(x))\n",
        "        out_query = self.Q_dropout(self.Q(x))\n",
        "        out_value = self.V_dropout(self.V(x))\n",
        "\n",
        "        # Calculate the score in different ways \n",
        "        s = 0\n",
        "        if self.attention_type == 'dot':\n",
        "            s = torch.bmm(out_key, out_query.view(x.shape[0], -1, 1))\n",
        "        elif self.attention_type == 'scaled_dot':\n",
        "            s = torch.bmm(out_key, out_query.view(x.shape[0], -1, 1)) / torch.sqrt(torch.tensor(x.shape[-1]))\n",
        "        elif self.attention_type == 'cos_sim':\n",
        "            s = torch.bmm(out_key, out_query.view(x.shape[0], -1, 1)) / (torch.norm(out_key, p=2) * torch.norm(out_query, p=2))\n",
        "\n",
        "        # Normalise the score \n",
        "        w = F.softmax(s, dim=0)\n",
        "\n",
        "        # Matmul Score and Value\n",
        "        y = torch.bmm(w, out_value)\n",
        "\n",
        "        return y"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l66ey8zSS4Ny"
      },
      "source": [
        "###Model(Baseline with CRF)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vf743sUaS6N-"
      },
      "outputs": [],
      "source": [
        "#Baseline model \n",
        "class Model(nn.Module):\n",
        "    def __init__(self, tag_to_ix, embedding_dim, hidden_dim, hidden_layer, dropout_rate, attention_layer, attention_type):\n",
        "        super(Model, self).__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.tagset_size = len(tag_to_ix)\n",
        "        self.attention_layer = attention_layer\n",
        "\n",
        "        # Word Embeddings \n",
        "        self.word_embeds = nn.Embedding(VOCAB_SIZE, embedding_dim)\n",
        "        self.word_embeds.weight.data.copy_(torch.from_numpy(embedding_matrix))\n",
        "\n",
        "        # Bi-LSTM Layers\n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_dim // 2, dropout=dropout_rate,\n",
        "                            num_layers=hidden_layer, bidirectional=True)\n",
        "\n",
        "        # CRF Layer \n",
        "        self.crf_layer = CRF_layer(tag_to_ix)\n",
        "\n",
        "        # Maps the output of the LSTM into tag space.\n",
        "        self.hidden2tag = nn.Linear(hidden_dim, self.tagset_size)\n",
        "\n",
        "    # Forward pass without CRF layer\n",
        "    def _forward_pass(self, sentence):\n",
        "        # Word Embeddings\n",
        "        word_embeds = self.word_embeds(sentence).view(len(sentence), 1, -1)\n",
        "        # LSTM Layers\n",
        "        lstm_out, _ = self.lstm(word_embeds)\n",
        "        att_out = lstm_out.view(len(sentence), self.hidden_dim)\n",
        "        feats = self.hidden2tag(att_out)\n",
        "        return feats\n",
        "\n",
        "    # Foward Probagation\n",
        "    def forward_training(self, sentence, tags):\n",
        "        feats = self._forward_pass(sentence)\n",
        "        forward_score = self.crf_layer.forward(feats)\n",
        "\n",
        "        # gold_score = self._score_sentence(feats, tags)\n",
        "        gold_score = self.crf_layer._score_sentence(feats, tags)\n",
        "\n",
        "        return forward_score - gold_score\n",
        "\n",
        "    def forward(self, sentence): \n",
        "        # Get the emission scores from the BiLSTM\n",
        "        feats = self._forward_pass(sentence)\n",
        "\n",
        "        # Find the best path, given the features.\n",
        "        score, tag_seq = self.crf_layer.decode(feats)\n",
        "        return score, tag_seq"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6LSrjTiiTIHC"
      },
      "outputs": [],
      "source": [
        "# Calculate f1, predicted, ground_truth, accuracy scores\n",
        "def cal_acc(model, input_index, output_index):\n",
        "    ground_truth = []\n",
        "    predicted = []\n",
        "    for i,idxs in enumerate(input_index):\n",
        "        ground_truth += output_index[i]\n",
        "        score, pred = model(torch.tensor(idxs, dtype=torch.long).to(device))\n",
        "        predicted += pred\n",
        "    accuracy = sum(np.array(ground_truth) == np.array(predicted))/len(ground_truth)\n",
        "    f1 = f1_score(ground_truth, predicted, average='weighted')\n",
        "    return predicted, ground_truth, accuracy, f1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j28KClZaTPpB"
      },
      "outputs": [],
      "source": [
        "def decode_output(output_list):\n",
        "    ix_to_tag = {v:k for k,v in tag_to_ix.items()}\n",
        "    return [ix_to_tag[output] for output in output_list]\n",
        "\n",
        "\n",
        "#Evaluate the result \n",
        "def evaluate_model(model):\n",
        "    y_pred, y_true, _, _ = cal_acc(model, val_input_index, val_output_index)\n",
        "    y_true_decode = decode_output(y_true)\n",
        "    y_pred_decode = decode_output(y_pred)\n",
        "\n",
        "    print(f'ground truth: {y_true_decode[:20]}')\n",
        "    print(f'prediction: {y_pred_decode[:20]}')\n",
        "\n",
        "    print(classification_report(y_true_decode, y_pred_decode, digits=4))\n",
        "\n",
        "\n",
        "def define_train_evaluate_model(model, num_epoch):\n",
        "    log = {\n",
        "        'train_loss': [],\n",
        "        'train_acc': [],\n",
        "        'train_f1': [],\n",
        "        'val_loss': [],\n",
        "        'val_acc': [],\n",
        "        'val_f1': [],\n",
        "    }\n",
        "\n",
        "    # Training Loop\n",
        "    for epoch in range(num_epoch):  \n",
        "        time1 = datetime.datetime.now()\n",
        "        train_loss = 0\n",
        "\n",
        "        model.train()\n",
        "        # Mini-Batch Training\n",
        "        for i, idxs in enumerate(train_input_index):\n",
        "            tags_index = train_output_index[i]\n",
        "\n",
        "            # Clear the gradients\n",
        "            model.zero_grad()\n",
        "\n",
        "            # Prepare the input and output data as tensors \n",
        "            sentence_in = torch.tensor(idxs, dtype=torch.long).to(device)\n",
        "            targets = torch.tensor(tags_index, dtype=torch.long).to(device)\n",
        "\n",
        "            # Forward propogation\n",
        "            loss = model.forward_training(sentence_in, targets)\n",
        "\n",
        "            # Backward Propogation\n",
        "            loss.backward()\n",
        "\n",
        "            # Update the model\n",
        "            optimizer.step()\n",
        "\n",
        "            # Accumulate the loss\n",
        "            train_loss+=loss.item()\n",
        "\n",
        "        model.eval()\n",
        "        _, _, train_acc, train_f1 = cal_acc(model,train_input_index,train_output_index)\n",
        "        _, _, val_acc, val_f1 = cal_acc(model,val_input_index,val_output_index)\n",
        "\n",
        "        # Evaluate on the validation dataset \n",
        "        val_loss = 0\n",
        "        for i, idxs in enumerate(val_input_index):\n",
        "            tags_index = val_output_index[i]\n",
        "            sentence_in = torch.tensor(idxs, dtype=torch.long).to(device)\n",
        "            targets = torch.tensor(tags_index, dtype=torch.long).to(device)\n",
        "            loss = model.forward_training(sentence_in, targets)\n",
        "            val_loss+=loss.item()\n",
        "        time2 = datetime.datetime.now()\n",
        "\n",
        "        # The log of each epoch\n",
        "        print(\"Epoch:%d, Training loss: %.2f, train acc: %.4f, train f1: %.4f, val loss: %.2f, val acc: %.4f, val f1: %.4f, time: %.2fs\" \\\n",
        "            %(epoch+1, train_loss, train_acc, train_f1, val_loss, val_acc, val_f1, (time2-time1).total_seconds()))\n",
        "        # Log the training log of each epoch\n",
        "        log['train_loss'].append(train_loss)\n",
        "        log['train_acc'].append(train_acc)\n",
        "        log['train_f1'].append(val_acc)\n",
        "        log['val_loss'].append(val_loss)\n",
        "        log['val_acc'].append(val_acc)\n",
        "        log['val_f1'].append(val_acc)\n",
        "\n",
        "    # Evaluate the model\n",
        "    evaluate_model(model)\n",
        "\n",
        "    return model, log"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F8wLGvOoTV8-",
        "outputId": "6ec22444-0a78-49a6-de5f-4d5af8f52fda"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Setup: \n",
            "Model(\n",
            "  (word_embeds): Embedding(11243, 25)\n",
            "  (lstm): LSTM(25, 25, bidirectional=True)\n",
            "  (crf_layer): CRF_layer()\n",
            "  (hidden2tag): Linear(in_features=50, out_features=12, bias=True)\n",
            ")\n",
            "\n",
            "Optimizer Setup: \n",
            "SGD (\n",
            "Parameter Group 0\n",
            "    dampening: 0\n",
            "    lr: 0.001\n",
            "    maximize: False\n",
            "    momentum: 0\n",
            "    nesterov: False\n",
            "    weight_decay: 0.0001\n",
            ")\n",
            "\n",
            "------ Start Training, Testing, Evaluating Process | Total Epoch: 2 ------\n",
            "Epoch:1, Training loss: 118108.62, train acc: 0.6750, train f1: 0.5440, val loss: 34559.88, val acc: 0.6772, val f1: 0.5469, time: 678.09s\n",
            "Epoch:2, Training loss: 81955.59, train acc: 0.8156, train f1: 0.7634, val loss: 18629.69, val acc: 0.8164, val f1: 0.7642, time: 688.29s\n",
            "Sample ground truth: ['s', 's', 's', 'o', 'o', 'o', 'o', 't', 'o', 'o', 'p', 'o', 'o', 'p', 'o', 't', 'o', 't', 'c', 'o']\n",
            "Sample prediction: ['s', 's', 'p', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'p', 'o', 'o', 'p', 'o', 'o', 'o', 'o', 'o', 'o']\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           c     0.0000    0.0000    0.0000      1641\n",
            "           d     0.0000    0.0000    0.0000       398\n",
            "           o     0.8521    1.0000    0.9201     22588\n",
            "           p     0.6746    0.7886    0.7272      3936\n",
            "           s     0.6853    0.4615    0.5515      3322\n",
            "           t     0.8333    0.0034    0.0068      1469\n",
            "\n",
            "    accuracy                         0.8164     33354\n",
            "   macro avg     0.5076    0.3756    0.3676     33354\n",
            "weighted avg     0.7616    0.8164    0.7642     33354\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ],
      "source": [
        "#LSTM\n",
        "HIDDEN_DIM = 50                # the number of hidden dimension on LSTM\n",
        "HIDDEN_LAYER = 1                # the number of hidden layer of LSTM\n",
        "\n",
        "# Attention\n",
        "ATTENTION_LAYER = 1             # the number of attention layer\n",
        "ATTENTION_TYPE = 'scaled_dot'   # dot | scaled_dot | cos_sim\n",
        "\n",
        "# Dropout rate on LSTM layers and Attention units\n",
        "DROPOUT_RATE = 0.0\n",
        "\n",
        "# Training\n",
        "NUM_EPOCH = 2\n",
        "LR = 0.001\n",
        "WEIGHT_DECAY = 1e-4\n",
        "\n",
        "# Create the model \n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model_base = Model(tag_to_ix, EMBEDDING_DIM, HIDDEN_DIM, HIDDEN_LAYER, DROPOUT_RATE, ATTENTION_LAYER, ATTENTION_TYPE).to(device)\n",
        "optimizer = optim.SGD(model_base.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)\n",
        "print(f'Model Setup: \\n{model_base}\\n')\n",
        "print(f'Optimizer Setup: \\n{optimizer}\\n')\n",
        "print(f'------ Start Training, Testing, Evaluating Process | Total Epoch: {NUM_EPOCH} ------')\n",
        "\n",
        "# Start Training, testing, evaluating process\n",
        "model_base, log = define_train_evaluate_model(model_base,\n",
        "                         num_epoch=NUM_EPOCH)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "je5DBMT_KDMc"
      },
      "source": [
        "###Model(Word2Vec+Bi-LSTM + CRF + Attention(Scaled Dot))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SYm1ZTPmKNlb",
        "outputId": "e326003b-88e7-4d70-8c8e-ae9698d5a459"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Setup: \n",
            "Model(\n",
            "  (word_embeds): Embedding(11243, 25)\n",
            "  (lstm): LSTM(25, 25, bidirectional=True)\n",
            "  (attention_layers): ModuleList(\n",
            "    (0): Attention_layer(\n",
            "      (K): Linear(in_features=50, out_features=50, bias=False)\n",
            "      (Q): Linear(in_features=50, out_features=50, bias=False)\n",
            "      (V): Linear(in_features=50, out_features=50, bias=False)\n",
            "      (K_dropout): Dropout(p=0.0, inplace=False)\n",
            "      (Q_dropout): Dropout(p=0.0, inplace=False)\n",
            "      (V_dropout): Dropout(p=0.0, inplace=False)\n",
            "    )\n",
            "  )\n",
            "  (crf_layer): CRF_layer()\n",
            "  (hidden2tag): Linear(in_features=50, out_features=12, bias=True)\n",
            ")\n",
            "\n",
            "Optimizer Setup: \n",
            "SGD (\n",
            "Parameter Group 0\n",
            "    dampening: 0\n",
            "    lr: 0.001\n",
            "    maximize: False\n",
            "    momentum: 0\n",
            "    nesterov: False\n",
            "    weight_decay: 0.0001\n",
            ")\n",
            "\n",
            "------ Start Training, Testing, Evaluating Process | Total Epoch: 2 ------\n",
            "Epoch:1, Training loss: 118322.22, train acc: 0.6750, train f1: 0.5440, val loss: 35182.16, val acc: 0.6772, val f1: 0.5469, time: 695.90s\n",
            "Epoch:2, Training loss: 100167.66, train acc: 0.7529, train f1: 0.6858, val loss: 28505.77, val acc: 0.7546, val f1: 0.6875, time: 690.41s\n",
            "Sample ground truth: ['s', 's', 's', 'o', 'o', 'o', 'o', 't', 'o', 'o', 'p', 'o', 'o', 'p', 'o', 't', 'o', 't', 'c', 'o']\n",
            "Sample prediction: ['s', 's', 'p', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'p', 'o', 'o', 'p', 'o', 'o', 'o', 'o', 'o', 'o']\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           c     0.0000    0.0000    0.0000      1641\n",
            "           d     0.0000    0.0000    0.0000       398\n",
            "           o     0.7446    1.0000    0.8536     22588\n",
            "           p     0.7797    0.3722    0.5039      3936\n",
            "           s     0.9867    0.3359    0.5012      3322\n",
            "           t     0.0000    0.0000    0.0000      1469\n",
            "\n",
            "    accuracy                         0.7546     33354\n",
            "   macro avg     0.4185    0.2847    0.3098     33354\n",
            "weighted avg     0.6946    0.7546    0.6875     33354\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ],
      "source": [
        "class Model(nn.Module):\n",
        "    def __init__(self, tag_to_ix, embedding_dim, hidden_dim, hidden_layer, dropout_rate, attention_layer, attention_type):\n",
        "        super(Model, self).__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.tagset_size = len(tag_to_ix)\n",
        "        self.attention_layer = attention_layer\n",
        "\n",
        "        # Word Embeddings \n",
        "        self.word_embeds = nn.Embedding(VOCAB_SIZE, embedding_dim)\n",
        "        self.word_embeds.weight.data.copy_(torch.from_numpy(embedding_matrix))\n",
        "\n",
        "        \n",
        "        # Bi-LSTM Layers\n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_dim // 2, dropout=dropout_rate,\n",
        "                            num_layers=hidden_layer, bidirectional=True)\n",
        "\n",
        "        # Attention Layers\n",
        "        self.attention_layers = nn.ModuleList()\n",
        "        for layer in range(attention_layer):\n",
        "            self.attention_layers.append(Attention_layer(embedding_dim+embedding_dim, attention_type, dropout_rate))\n",
        "\n",
        "        # CRF Layer \n",
        "        self.crf_layer = CRF_layer(tag_to_ix)\n",
        "\n",
        "        # Maps the output of the LSTM into tag space.\n",
        "        self.hidden2tag = nn.Linear(hidden_dim, self.tagset_size)\n",
        "\n",
        "    # Forward pass without CRF layer\n",
        "    def _forward_pass(self, sentence):\n",
        "        # Word Embeddings\n",
        "        word_embeds = self.word_embeds(sentence).view(len(sentence), 1, -1)\n",
        "\n",
        "        # LSTM Layers\n",
        "        lstm_out, _ = self.lstm(word_embeds)\n",
        "\n",
        "        # Attention Calculation\n",
        "        attention_score = lstm_out\n",
        "        for layer in self.attention_layers[:-1]:\n",
        "            attention_score = layer(attention_score)\n",
        "\n",
        "        att_out = lstm_out.view(len(sentence), self.hidden_dim)\n",
        "        feats = self.hidden2tag(att_out)\n",
        "\n",
        "        return feats\n",
        "\n",
        "    # Use where Forward Propogation during training\n",
        "    def forward_training(self, sentence, tags):\n",
        "        feats = self._forward_pass(sentence)\n",
        "        forward_score = self.crf_layer.forward(feats)\n",
        "\n",
        "        # gold_score = self._score_sentence(feats, tags)\n",
        "        gold_score = self.crf_layer._score_sentence(feats, tags)\n",
        "\n",
        "        return forward_score - gold_score\n",
        "\n",
        "    def forward(self, sentence): \n",
        "        # Get the emission scores from the BiLSTM\n",
        "        feats = self._forward_pass(sentence)\n",
        "\n",
        "        score, tag_seq = self.crf_layer.decode(feats)\n",
        "        return score, tag_seq\n",
        "\n",
        "def cal_acc(model, input_index, output_index):\n",
        "    ground_truth = []\n",
        "    predicted = []\n",
        "    for i,idxs in enumerate(input_index):\n",
        "        ground_truth += output_index[i]\n",
        "        score, pred = model(torch.tensor(idxs, dtype=torch.long).to(device))\n",
        "        predicted += pred\n",
        "    accuracy = sum(np.array(ground_truth) == np.array(predicted))/len(ground_truth)\n",
        "    f1 = f1_score(ground_truth, predicted, average='weighted')\n",
        "    return predicted, ground_truth, accuracy, f1\n",
        "\n",
        "\n",
        "def decode_output(output_list):\n",
        "    ix_to_tag = {v:k for k,v in tag_to_ix.items()}\n",
        "    return [ix_to_tag[output] for output in output_list]\n",
        "\n",
        "\n",
        "#Sample and classification report\n",
        "def evaluate_model(model):\n",
        "    # Get the results\n",
        "    y_pred, y_true, _, _ = cal_acc(model, val_input_index, val_output_index)\n",
        "    y_true_decode = decode_output(y_true)\n",
        "    y_pred_decode = decode_output(y_pred)\n",
        "\n",
        "    print(f'Sample ground truth: {y_true_decode[:20]}')\n",
        "    print(f'Sample prediction: {y_pred_decode[:20]}')\n",
        "\n",
        "    print(classification_report(y_true_decode, y_pred_decode, digits=4))\n",
        "\n",
        "\n",
        "def define_train_evaluate_model(model, num_epoch=10):\n",
        "    # For recording the training log for visualisation\n",
        "    log = {\n",
        "        'train_loss': [],\n",
        "        'train_acc': [],\n",
        "        'train_f1': [],\n",
        "        'val_loss': [],\n",
        "        'val_acc': [],\n",
        "        'val_f1': [],\n",
        "    }\n",
        "\n",
        "    # Training Loop\n",
        "    for epoch in range(num_epoch):  \n",
        "        time1 = datetime.datetime.now()\n",
        "        train_loss = 0\n",
        "\n",
        "        model.train()\n",
        "        # Mini-Batch Training\n",
        "        for i, idxs in enumerate(train_input_index):\n",
        "            tags_index = train_output_index[i]\n",
        "\n",
        "            # Clear the gradients\n",
        "            model.zero_grad()\n",
        "\n",
        "            # Prepare the input and output data as tensors \n",
        "            sentence_in = torch.tensor(idxs, dtype=torch.long).to(device)\n",
        "            targets = torch.tensor(tags_index, dtype=torch.long).to(device)\n",
        "\n",
        "            # Forward propogation\n",
        "            loss = model.forward_training(sentence_in, targets)\n",
        "\n",
        "            # Backward Propogation\n",
        "            loss.backward()\n",
        "\n",
        "            # Update the model\n",
        "            optimizer.step()\n",
        "\n",
        "            # Accumulate the loss\n",
        "            train_loss+=loss.item()\n",
        "\n",
        "        # Evaluate the model on train_set and val_set\n",
        "        model.eval()\n",
        "        _, _, train_acc, train_f1 = cal_acc(model,train_input_index,train_output_index)\n",
        "        _, _, val_acc, val_f1 = cal_acc(model,val_input_index,val_output_index)\n",
        "\n",
        "        # Evaluate on the validation dataset \n",
        "        val_loss = 0\n",
        "        for i, idxs in enumerate(val_input_index):\n",
        "            tags_index = val_output_index[i]\n",
        "            sentence_in = torch.tensor(idxs, dtype=torch.long).to(device)\n",
        "            targets = torch.tensor(tags_index, dtype=torch.long).to(device)\n",
        "            loss = model.forward_training(sentence_in, targets)\n",
        "            val_loss+=loss.item()\n",
        "        time2 = datetime.datetime.now()\n",
        "\n",
        "        # The log of each epoch\n",
        "        print(\"Epoch:%d, Training loss: %.2f, train acc: %.4f, train f1: %.4f, val loss: %.2f, val acc: %.4f, val f1: %.4f, time: %.2fs\" \\\n",
        "            %(epoch+1, train_loss, train_acc, train_f1, val_loss, val_acc, val_f1, (time2-time1).total_seconds()))\n",
        "        # Log the training log of each epoch\n",
        "        log['train_loss'].append(train_loss)\n",
        "        log['train_acc'].append(train_acc)\n",
        "        log['train_f1'].append(val_acc)\n",
        "        log['val_loss'].append(val_loss)\n",
        "        log['val_acc'].append(val_acc)\n",
        "        log['val_f1'].append(val_acc)\n",
        "\n",
        "    # Evaluate the model\n",
        "    evaluate_model(model)\n",
        "\n",
        "    return model, log\n",
        "# LSTM\n",
        "HIDDEN_DIM = 50                # the number of hidden dimension on LSTM\n",
        "HIDDEN_LAYER = 1                # the number of hidden layer of LSTM\n",
        "\n",
        "# Attention\n",
        "ATTENTION_LAYER = 1             # the number of attention layer\n",
        "ATTENTION_TYPE = 'scaled_dot'   # dot | scaled_dot | cos_sim\n",
        "\n",
        "# Dropout rate on LSTM layers and Attention units\n",
        "DROPOUT_RATE = 0.0\n",
        "\n",
        "# Training\n",
        "NUM_EPOCH = 2\n",
        "LR = 0.001\n",
        "WEIGHT_DECAY = 1e-4\n",
        "\n",
        "# Create the model \n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model_wv_ls_crf_att = Model(tag_to_ix, EMBEDDING_DIM, HIDDEN_DIM, HIDDEN_LAYER, DROPOUT_RATE, ATTENTION_LAYER, ATTENTION_TYPE).to(device)\n",
        "optimizer = optim.SGD(model_wv_ls_crf_att.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)\n",
        "print(f'Model Setup: \\n{model_wv_ls_crf_att}\\n')\n",
        "print(f'Optimizer Setup: \\n{optimizer}\\n')\n",
        "print(f'------ Start Training, Testing, Evaluating Process | Total Epoch: {NUM_EPOCH} ------')\n",
        "\n",
        "# Start Training, testing, evaluating process\n",
        "model_wv_ls_crf_att, log = define_train_evaluate_model(model_wv_ls_crf_att,\n",
        "                         num_epoch=NUM_EPOCH)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Model(Stacked layers(2) Bi-LSTM + Word2Vec + CRF + Attention(dot))"
      ],
      "metadata": {
        "id": "s9SJv0kuJaIy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Model(nn.Module):\n",
        "    def __init__(self, tag_to_ix, embedding_dim, hidden_dim, hidden_layer, dropout_rate, attention_layer, attention_type):\n",
        "        super(Model, self).__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.tagset_size = len(tag_to_ix)\n",
        "        self.attention_layer = attention_layer\n",
        "\n",
        "        # Word Embeddings \n",
        "        self.word_embeds = nn.Embedding(VOCAB_SIZE, embedding_dim)\n",
        "        self.word_embeds.weight.data.copy_(torch.from_numpy(embedding_matrix))\n",
        "\n",
        "        # Pos Embeddings\n",
        "        # self.pos_embeds = nn.Embedding(VOCAB_SIZE, POSTAG_SIZE)\n",
        "        # self.pos_embeds.weight.data.copy_(torch.from_numpy(pos_embedding_matrix))\n",
        "        # self.pos_linear = nn.Linear(POSTAG_SIZE, POSTAG_SIZE)\n",
        "\n",
        "        # Bi-LSTM Layers\n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_dim // 2, dropout=dropout_rate,\n",
        "                            num_layers=hidden_layer, bidirectional=True)\n",
        "        \n",
        "        # Attention Layers\n",
        "        self.attention_layers = nn.ModuleList()\n",
        "        for layer in range(attention_layer):\n",
        "            self.attention_layers.append(Attention_layer(embedding_dim+embedding_dim, attention_type, dropout_rate))\n",
        "\n",
        "        # CRF Layer \n",
        "        self.crf_layer = CRF_layer(tag_to_ix)\n",
        "\n",
        "        # Maps the output of the LSTM into tag space.\n",
        "        self.hidden2tag = nn.Linear(hidden_dim, self.tagset_size)\n",
        "\n",
        "    # Forward pass without CRF layer\n",
        "    def _forward_pass(self, sentence):\n",
        "        # Word Embeddings\n",
        "        word_embeds = self.word_embeds(sentence).view(len(sentence), 1, -1)\n",
        "\n",
        "        # PoS Tag Embeddings\n",
        "        # pos_embeds = self.pos_embeds(sentence).view(len(sentence), 1, -1)\n",
        "        # pos_embeds = self.pos_linear(pos_embeds)\n",
        "\n",
        "        # Concated Embeddings\n",
        "        # embeds = torch.cat((word_embeds, pos_embeds), 2)\n",
        "\n",
        "        # LSTM Layers\n",
        "        lstm_out, _ = self.lstm(word_embeds)\n",
        "\n",
        "        # Attention Calculation\n",
        "        attention_score = lstm_out\n",
        "        for layer in self.attention_layers[:-1]:\n",
        "            attention_score = layer(attention_score)\n",
        "\n",
        "        att_out = lstm_out.view(len(sentence), self.hidden_dim)\n",
        "        feats = self.hidden2tag(att_out)\n",
        "\n",
        "        return feats\n",
        "\n",
        "    # Use where Forward Propogation during training\n",
        "    def forward_training(self, sentence, tags):\n",
        "        feats = self._forward_pass(sentence)\n",
        "        forward_score = self.crf_layer.forward(feats)\n",
        "\n",
        "        # gold_score = self._score_sentence(feats, tags)\n",
        "        gold_score = self.crf_layer._score_sentence(feats, tags)\n",
        "\n",
        "        return forward_score - gold_score\n",
        "\n",
        "    # Use for inference\n",
        "    def forward(self, sentence): \n",
        "        # Get the emission scores from the BiLSTM\n",
        "        feats = self._forward_pass(sentence)\n",
        "\n",
        "        # Find the best path, given the features.\n",
        "        score, tag_seq = self.crf_layer.decode(feats)\n",
        "        return score, tag_seq\n",
        "# Calculate the scores given ground truth and prediction\n",
        "def cal_acc(model, input_index, output_index):\n",
        "    ground_truth = []\n",
        "    predicted = []\n",
        "    for i,idxs in enumerate(input_index):\n",
        "        ground_truth += output_index[i]\n",
        "        score, pred = model(torch.tensor(idxs, dtype=torch.long).to(device))\n",
        "        predicted += pred\n",
        "    accuracy = sum(np.array(ground_truth) == np.array(predicted))/len(ground_truth)\n",
        "    f1 = f1_score(ground_truth, predicted, average='weighted')\n",
        "    return predicted, ground_truth, accuracy, f1\n",
        "\n",
        "\n",
        "\n",
        "# Map the tag index to NER tag name\n",
        "def decode_output(output_list):\n",
        "    ix_to_tag = {v:k for k,v in tag_to_ix.items()}\n",
        "    return [ix_to_tag[output] for output in output_list]\n",
        "\n",
        "\n",
        "# Print out the prediction sample and classification report\n",
        "def evaluate_model(model):\n",
        "    # Get the results\n",
        "    y_pred, y_true, _, _ = cal_acc(model, val_input_index, val_output_index)\n",
        "    y_true_decode = decode_output(y_true)\n",
        "    y_pred_decode = decode_output(y_pred)\n",
        "\n",
        "    print(f'Sample ground truth: {y_true_decode[:20]}')\n",
        "    print(f'Sample prediction: {y_pred_decode[:20]}')\n",
        "\n",
        "    print(classification_report(y_true_decode, y_pred_decode, digits=4))\n",
        "\n",
        "\n",
        "def define_train_evaluate_model(model, num_epoch=10):\n",
        "    # For recording the training log for visualisation\n",
        "    log = {\n",
        "        'train_loss': [],\n",
        "        'train_acc': [],\n",
        "        'train_f1': [],\n",
        "        'val_loss': [],\n",
        "        'val_acc': [],\n",
        "        'val_f1': [],\n",
        "    }\n",
        "\n",
        "    # Training Loop\n",
        "    for epoch in range(num_epoch):  \n",
        "        time1 = datetime.datetime.now()\n",
        "        train_loss = 0\n",
        "\n",
        "        model.train()\n",
        "        # Mini-Batch Training\n",
        "        for i, idxs in enumerate(train_input_index):\n",
        "            tags_index = train_output_index[i]\n",
        "\n",
        "            # Clear the gradients\n",
        "            model.zero_grad()\n",
        "\n",
        "            # Prepare the input and output data as tensors \n",
        "            sentence_in = torch.tensor(idxs, dtype=torch.long).to(device)\n",
        "            targets = torch.tensor(tags_index, dtype=torch.long).to(device)\n",
        "\n",
        "            # Forward propogation\n",
        "            loss = model.forward_training(sentence_in, targets)\n",
        "\n",
        "            # Backward Propogation\n",
        "            loss.backward()\n",
        "\n",
        "            # Update the model\n",
        "            optimizer.step()\n",
        "\n",
        "            # Accumulate the loss\n",
        "            train_loss+=loss.item()\n",
        "\n",
        "        # Evaluate the model on train_set and val_set\n",
        "        model.eval()\n",
        "        _, _, train_acc, train_f1 = cal_acc(model,train_input_index,train_output_index)\n",
        "        _, _, val_acc, val_f1 = cal_acc(model,val_input_index,val_output_index)\n",
        "\n",
        "        # Evaluate on the validation dataset \n",
        "        val_loss = 0\n",
        "        for i, idxs in enumerate(val_input_index):\n",
        "            tags_index = val_output_index[i]\n",
        "            sentence_in = torch.tensor(idxs, dtype=torch.long).to(device)\n",
        "            targets = torch.tensor(tags_index, dtype=torch.long).to(device)\n",
        "            loss = model.forward_training(sentence_in, targets)\n",
        "            val_loss+=loss.item()\n",
        "        time2 = datetime.datetime.now()\n",
        "\n",
        "        # The log of each epoch\n",
        "        print(\"Epoch:%d, Training loss: %.2f, train acc: %.4f, train f1: %.4f, val loss: %.2f, val acc: %.4f, val f1: %.4f, time: %.2fs\" \\\n",
        "            %(epoch+1, train_loss, train_acc, train_f1, val_loss, val_acc, val_f1, (time2-time1).total_seconds()))\n",
        "        # Log the training log of each epoch\n",
        "        log['train_loss'].append(train_loss)\n",
        "        log['train_acc'].append(train_acc)\n",
        "        log['train_f1'].append(val_acc)\n",
        "        log['val_loss'].append(val_loss)\n",
        "        log['val_acc'].append(val_acc)\n",
        "        log['val_f1'].append(val_acc)\n",
        "\n",
        "    # Evaluate the model\n",
        "    evaluate_model(model)\n",
        "\n",
        "    return model, log\n",
        "# LSTM\n",
        "HIDDEN_DIM = 50               \n",
        "HIDDEN_LAYER = 2                \n",
        "\n",
        "# Attention\n",
        "ATTENTION_LAYER = 1             \n",
        "ATTENTION_TYPE = 'scaled_dot'   # dot | scaled_dot | cos_sim\n",
        "\n",
        "# Dropout rate on LSTM layers and Attention units\n",
        "DROPOUT_RATE = 0.0\n",
        "\n",
        "# Training\n",
        "NUM_EPOCH = 2\n",
        "LR = 0.001\n",
        "WEIGHT_DECAY = 1e-4\n",
        "\n",
        "# Create the model \n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model_lstm2 = Model(tag_to_ix, EMBEDDING_DIM, HIDDEN_DIM, HIDDEN_LAYER, DROPOUT_RATE, ATTENTION_LAYER, ATTENTION_TYPE).to(device)\n",
        "optimizer = optim.SGD(model_lstm2.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)\n",
        "print(f'Model Setup: \\n{model_lstm2}\\n')\n",
        "print(f'Optimizer Setup: \\n{optimizer}\\n')\n",
        "print(f'------ Start Training, Testing, Evaluating Process | Total Epoch: {NUM_EPOCH} ------')\n",
        "\n",
        "# Start Training, testing, evaluating process\n",
        "model_lstm2, log = define_train_evaluate_model(model_lstm2,\n",
        "                         num_epoch=NUM_EPOCH)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3o789d-qJ2PU",
        "outputId": "45cbca46-308f-4eed-a348-b8b03e4e31d2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Setup: \n",
            "Model(\n",
            "  (word_embeds): Embedding(11243, 25)\n",
            "  (lstm): LSTM(25, 25, num_layers=2, bidirectional=True)\n",
            "  (attention_layers): ModuleList(\n",
            "    (0): Attention_layer(\n",
            "      (K): Linear(in_features=50, out_features=50, bias=False)\n",
            "      (Q): Linear(in_features=50, out_features=50, bias=False)\n",
            "      (V): Linear(in_features=50, out_features=50, bias=False)\n",
            "      (K_dropout): Dropout(p=0.0, inplace=False)\n",
            "      (Q_dropout): Dropout(p=0.0, inplace=False)\n",
            "      (V_dropout): Dropout(p=0.0, inplace=False)\n",
            "    )\n",
            "  )\n",
            "  (crf_layer): CRF_layer()\n",
            "  (hidden2tag): Linear(in_features=50, out_features=12, bias=True)\n",
            ")\n",
            "\n",
            "Optimizer Setup: \n",
            "SGD (\n",
            "Parameter Group 0\n",
            "    dampening: 0\n",
            "    lr: 0.001\n",
            "    maximize: False\n",
            "    momentum: 0\n",
            "    nesterov: False\n",
            "    weight_decay: 0.0001\n",
            ")\n",
            "\n",
            "------ Start Training, Testing, Evaluating Process | Total Epoch: 2 ------\n",
            "Epoch:1, Training loss: 115341.11, train acc: 0.6750, train f1: 0.5440, val loss: 35643.97, val acc: 0.6772, val f1: 0.5469, time: 765.91s\n",
            "Epoch:2, Training loss: 105682.07, train acc: 0.6750, train f1: 0.5440, val loss: 35050.61, val acc: 0.6772, val f1: 0.5469, time: 777.45s\n",
            "Sample ground truth: ['s', 's', 's', 'o', 'o', 'o', 'o', 't', 'o', 'o', 'p', 'o', 'o', 'p', 'o', 't', 'o', 't', 'c', 'o']\n",
            "Sample prediction: ['o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o']\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           c     0.0000    0.0000    0.0000      1641\n",
            "           d     0.0000    0.0000    0.0000       398\n",
            "           o     0.6772    1.0000    0.8076     22588\n",
            "           p     0.0000    0.0000    0.0000      3936\n",
            "           s     0.0000    0.0000    0.0000      3322\n",
            "           t     0.0000    0.0000    0.0000      1469\n",
            "\n",
            "    accuracy                         0.6772     33354\n",
            "   macro avg     0.1129    0.1667    0.1346     33354\n",
            "weighted avg     0.4586    0.6772    0.5469     33354\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Model(Word2Vec + Bi-LSTM + CRF + Attention(dot)"
      ],
      "metadata": {
        "id": "sQhISC1AHfrG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Model(nn.Module):\n",
        "    def __init__(self, tag_to_ix, embedding_dim, hidden_dim, hidden_layer, dropout_rate, attention_layer, attention_type):\n",
        "        super(Model, self).__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.tagset_size = len(tag_to_ix)\n",
        "        self.attention_layer = attention_layer\n",
        "\n",
        "        # Word Embeddings \n",
        "        self.word_embeds = nn.Embedding(VOCAB_SIZE, embedding_dim)\n",
        "        self.word_embeds.weight.data.copy_(torch.from_numpy(embedding_matrix))\n",
        "\n",
        "        # Pos Embeddings\n",
        "        # self.pos_embeds = nn.Embedding(VOCAB_SIZE, POSTAG_SIZE)\n",
        "        # self.pos_embeds.weight.data.copy_(torch.from_numpy(pos_embedding_matrix))\n",
        "        # self.pos_linear = nn.Linear(POSTAG_SIZE, POSTAG_SIZE)\n",
        "\n",
        "        # Bi-LSTM Layers\n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_dim // 2, dropout=dropout_rate,\n",
        "                            num_layers=hidden_layer, bidirectional=True)\n",
        "\n",
        "        # Attention Layers\n",
        "        self.attention_layers = nn.ModuleList()\n",
        "        for layer in range(attention_layer):\n",
        "            self.attention_layers.append(Attention_layer(embedding_dim+embedding_dim, attention_type, dropout_rate))\n",
        "\n",
        "        # CRF Layer \n",
        "        self.crf_layer = CRF_layer(tag_to_ix)\n",
        "\n",
        "        # Maps the output of the LSTM into tag space.\n",
        "        self.hidden2tag = nn.Linear(hidden_dim, self.tagset_size)\n",
        "\n",
        "    # Forward pass without CRF layer\n",
        "    def _forward_pass(self, sentence):\n",
        "        # Word Embeddings\n",
        "        word_embeds = self.word_embeds(sentence).view(len(sentence), 1, -1)\n",
        "\n",
        "        # PoS Tag Embeddings\n",
        "        # pos_embeds = self.pos_embeds(sentence).view(len(sentence), 1, -1)\n",
        "        # pos_embeds = self.pos_linear(pos_embeds)\n",
        "\n",
        "        # Concated Embeddings\n",
        "        # embeds = torch.cat((word_embeds, pos_embeds), 2)\n",
        "\n",
        "        # LSTM Layers\n",
        "        lstm_out, _ = self.lstm(word_embeds)\n",
        "\n",
        "        # Attention Calculation\n",
        "        attention_score = lstm_out\n",
        "        for layer in self.attention_layers[:-1]:\n",
        "            attention_score = layer(attention_score)\n",
        "\n",
        "        att_out = lstm_out.view(len(sentence), self.hidden_dim)\n",
        "        feats = self.hidden2tag(att_out)\n",
        "\n",
        "        return feats\n",
        "\n",
        "    # Use where Forward Propogation during training\n",
        "    def forward_training(self, sentence, tags):\n",
        "        feats = self._forward_pass(sentence)\n",
        "        forward_score = self.crf_layer.forward(feats)\n",
        "\n",
        "        # gold_score = self._score_sentence(feats, tags)\n",
        "        gold_score = self.crf_layer._score_sentence(feats, tags)\n",
        "\n",
        "        return forward_score - gold_score\n",
        "\n",
        "    # Use for inference\n",
        "    def forward(self, sentence): \n",
        "        # Get the emission scores from the BiLSTM\n",
        "        feats = self._forward_pass(sentence)\n",
        "\n",
        "        # Find the best path, given the features.\n",
        "        score, tag_seq = self.crf_layer.decode(feats)\n",
        "        return score, tag_seq\n",
        "# Calculate the scores given ground truth and prediction\n",
        "def cal_acc(model, input_index, output_index):\n",
        "    ground_truth = []\n",
        "    predicted = []\n",
        "    for i,idxs in enumerate(input_index):\n",
        "        ground_truth += output_index[i]\n",
        "        score, pred = model(torch.tensor(idxs, dtype=torch.long).to(device))\n",
        "        predicted += pred\n",
        "    accuracy = sum(np.array(ground_truth) == np.array(predicted))/len(ground_truth)\n",
        "    f1 = f1_score(ground_truth, predicted, average='weighted')\n",
        "    return predicted, ground_truth, accuracy, f1\n",
        "\n",
        "\n",
        "\n",
        "# Map the tag index to NER tag name\n",
        "def decode_output(output_list):\n",
        "    ix_to_tag = {v:k for k,v in tag_to_ix.items()}\n",
        "    return [ix_to_tag[output] for output in output_list]\n",
        "\n",
        "\n",
        "# Print out the prediction sample and classification report\n",
        "def evaluate_model(model):\n",
        "    # Get the results\n",
        "    y_pred, y_true, _, _ = cal_acc(model, val_input_index, val_output_index)\n",
        "    y_true_decode = decode_output(y_true)\n",
        "    y_pred_decode = decode_output(y_pred)\n",
        "\n",
        "    print(f'Sample ground truth: {y_true_decode[:20]}')\n",
        "    print(f'Sample prediction: {y_pred_decode[:20]}')\n",
        "\n",
        "    print(classification_report(y_true_decode, y_pred_decode, digits=4))\n",
        "\n",
        "\n",
        "def define_train_evaluate_model(model, num_epoch=10):\n",
        "    # For recording the training log for visualisation\n",
        "    log = {\n",
        "        'train_loss': [],\n",
        "        'train_acc': [],\n",
        "        'train_f1': [],\n",
        "        'val_loss': [],\n",
        "        'val_acc': [],\n",
        "        'val_f1': [],\n",
        "    }\n",
        "\n",
        "    # Training Loop\n",
        "    for epoch in range(num_epoch):  \n",
        "        time1 = datetime.datetime.now()\n",
        "        train_loss = 0\n",
        "\n",
        "        model.train()\n",
        "        # Mini-Batch Training\n",
        "        for i, idxs in enumerate(train_input_index):\n",
        "            tags_index = train_output_index[i]\n",
        "\n",
        "            # Clear the gradients\n",
        "            model.zero_grad()\n",
        "\n",
        "            # Prepare the input and output data as tensors \n",
        "            sentence_in = torch.tensor(idxs, dtype=torch.long).to(device)\n",
        "            targets = torch.tensor(tags_index, dtype=torch.long).to(device)\n",
        "\n",
        "            # Forward propogation\n",
        "            loss = model.forward_training(sentence_in, targets)\n",
        "\n",
        "            # Backward Propogation\n",
        "            loss.backward()\n",
        "\n",
        "            # Update the model\n",
        "            optimizer.step()\n",
        "\n",
        "            # Accumulate the loss\n",
        "            train_loss+=loss.item()\n",
        "\n",
        "        # Evaluate the model on train_set and val_set\n",
        "        model.eval()\n",
        "        _, _, train_acc, train_f1 = cal_acc(model,train_input_index,train_output_index)\n",
        "        _, _, val_acc, val_f1 = cal_acc(model,val_input_index,val_output_index)\n",
        "\n",
        "        # Evaluate on the validation dataset \n",
        "        val_loss = 0\n",
        "        for i, idxs in enumerate(val_input_index):\n",
        "            tags_index = val_output_index[i]\n",
        "            sentence_in = torch.tensor(idxs, dtype=torch.long).to(device)\n",
        "            targets = torch.tensor(tags_index, dtype=torch.long).to(device)\n",
        "            loss = model.forward_training(sentence_in, targets)\n",
        "            val_loss+=loss.item()\n",
        "        time2 = datetime.datetime.now()\n",
        "\n",
        "        # The log of each epoch\n",
        "        print(\"Epoch:%d, Training loss: %.2f, train acc: %.4f, train f1: %.4f, val loss: %.2f, val acc: %.4f, val f1: %.4f, time: %.2fs\" \\\n",
        "            %(epoch+1, train_loss, train_acc, train_f1, val_loss, val_acc, val_f1, (time2-time1).total_seconds()))\n",
        "        # Log the training log of each epoch\n",
        "        log['train_loss'].append(train_loss)\n",
        "        log['train_acc'].append(train_acc)\n",
        "        log['train_f1'].append(val_acc)\n",
        "        log['val_loss'].append(val_loss)\n",
        "        log['val_acc'].append(val_acc)\n",
        "        log['val_f1'].append(val_acc)\n",
        "\n",
        "    # Evaluate the model\n",
        "    evaluate_model(model)\n",
        "\n",
        "    return model, log\n",
        "# LSTM\n",
        "HIDDEN_DIM = 50                # the number of hidden dimension on LSTM\n",
        "HIDDEN_LAYER = 1                # the number of hidden layer of LSTM\n",
        "\n",
        "# Attention\n",
        "ATTENTION_LAYER = 1             # the number of attention layer\n",
        "ATTENTION_TYPE = 'dot'   # dot | scaled_dot | cos_sim\n",
        "\n",
        "# Dropout rate on LSTM layers and Attention units\n",
        "DROPOUT_RATE = 0.0\n",
        "\n",
        "# Training\n",
        "NUM_EPOCH = 2\n",
        "LR = 0.001\n",
        "WEIGHT_DECAY = 1e-4\n",
        "\n",
        "# Create the model \n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model_att_dot = Model(tag_to_ix, EMBEDDING_DIM, HIDDEN_DIM, HIDDEN_LAYER, DROPOUT_RATE, ATTENTION_LAYER, ATTENTION_TYPE).to(device)\n",
        "optimizer = optim.SGD(model_att_dot.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)\n",
        "print(f'Model Setup: \\n{model_att_dot}\\n')\n",
        "print(f'Optimizer Setup: \\n{optimizer}\\n')\n",
        "print(f'------ Start Training, Testing, Evaluating Process | Total Epoch: {NUM_EPOCH} ------')\n",
        "\n",
        "# Start Training, testing, evaluating process\n",
        "model_att_dot, log = define_train_evaluate_model(model_att_dot,\n",
        "                         num_epoch=NUM_EPOCH)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hE2JjklYHtIs",
        "outputId": "94835e4a-04d6-4714-cd13-1d0c6ba111a2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Setup: \n",
            "Model(\n",
            "  (word_embeds): Embedding(11243, 25)\n",
            "  (lstm): LSTM(25, 25, bidirectional=True)\n",
            "  (attention_layers): ModuleList(\n",
            "    (0): Attention_layer(\n",
            "      (K): Linear(in_features=50, out_features=50, bias=False)\n",
            "      (Q): Linear(in_features=50, out_features=50, bias=False)\n",
            "      (V): Linear(in_features=50, out_features=50, bias=False)\n",
            "      (K_dropout): Dropout(p=0.0, inplace=False)\n",
            "      (Q_dropout): Dropout(p=0.0, inplace=False)\n",
            "      (V_dropout): Dropout(p=0.0, inplace=False)\n",
            "    )\n",
            "  )\n",
            "  (crf_layer): CRF_layer()\n",
            "  (hidden2tag): Linear(in_features=50, out_features=12, bias=True)\n",
            ")\n",
            "\n",
            "Optimizer Setup: \n",
            "SGD (\n",
            "Parameter Group 0\n",
            "    dampening: 0\n",
            "    lr: 0.001\n",
            "    maximize: False\n",
            "    momentum: 0\n",
            "    nesterov: False\n",
            "    weight_decay: 0.0001\n",
            ")\n",
            "\n",
            "------ Start Training, Testing, Evaluating Process | Total Epoch: 2 ------\n",
            "Epoch:1, Training loss: 114819.33, train acc: 0.6750, train f1: 0.5440, val loss: 35316.66, val acc: 0.6772, val f1: 0.5469, time: 754.45s\n",
            "Epoch:2, Training loss: 101253.54, train acc: 0.7432, train f1: 0.6713, val loss: 29255.42, val acc: 0.7448, val f1: 0.6730, time: 746.64s\n",
            "Sample ground truth: ['s', 's', 's', 'o', 'o', 'o', 'o', 't', 'o', 'o', 'p', 'o', 'o', 'p', 'o', 't', 'o', 't', 'c', 'o']\n",
            "Sample prediction: ['s', 's', 'p', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'p', 'o', 'o', 'p', 'o', 'o', 'o', 'o', 'o', 'o']\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           c     0.0000    0.0000    0.0000      1641\n",
            "           d     0.0000    0.0000    0.0000       398\n",
            "           o     0.7374    1.0000    0.8489     22588\n",
            "           p     0.7441    0.3369    0.4638      3936\n",
            "           s     0.9862    0.2793    0.4354      3322\n",
            "           t     0.0000    0.0000    0.0000      1469\n",
            "\n",
            "    accuracy                         0.7448     33354\n",
            "   macro avg     0.4113    0.2694    0.2913     33354\n",
            "weighted avg     0.6854    0.7448    0.6730     33354\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Print out the result \n",
        "model_att_dot.eval()\n",
        "with torch.no_grad():\n",
        "    predicted = []\n",
        "\n",
        "    for i,idxs in enumerate(test_input_index):\n",
        "        score, pred = model_att_dot(torch.tensor(idxs, dtype=torch.long).to(device))\n",
        "        predicted += pred\n",
        "        \n",
        "test_pred_decode = decode_output(predicted)\n",
        "id = []\n",
        "for i,item in enumerate(test_pred_decode):\n",
        "    id.append(i)\n",
        "\n",
        "df = pd.DataFrame({'ID':id,'Predicted':test_pred_decode})\n",
        "df.to_csv(r\"result_final.csv\",index=False)"
      ],
      "metadata": {
        "id": "wB5hCYA78Vk_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Model(Word2Vec + Bi-LSTM + CRF + Attention(cos similarity)"
      ],
      "metadata": {
        "id": "M6BOcqX-Hym4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Model(nn.Module):\n",
        "    def __init__(self, tag_to_ix, embedding_dim, hidden_dim, hidden_layer, dropout_rate, attention_layer, attention_type):\n",
        "        super(Model, self).__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.tagset_size = len(tag_to_ix)\n",
        "        self.attention_layer = attention_layer\n",
        "\n",
        "        # Word Embeddings \n",
        "        self.word_embeds = nn.Embedding(VOCAB_SIZE, embedding_dim)\n",
        "        self.word_embeds.weight.data.copy_(torch.from_numpy(embedding_matrix))\n",
        "\n",
        "        # Pos Embeddings\n",
        "        # self.pos_embeds = nn.Embedding(VOCAB_SIZE, POSTAG_SIZE)\n",
        "        # self.pos_embeds.weight.data.copy_(torch.from_numpy(pos_embedding_matrix))\n",
        "        # self.pos_linear = nn.Linear(POSTAG_SIZE, POSTAG_SIZE)\n",
        "\n",
        "        # Bi-LSTM Layers\n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_dim // 2, dropout=dropout_rate,\n",
        "                            num_layers=hidden_layer, bidirectional=True)\n",
        "\n",
        "        # Attention Layers\n",
        "        self.attention_layers = nn.ModuleList()\n",
        "        for layer in range(attention_layer):\n",
        "            self.attention_layers.append(Attention_layer(embedding_dim+embedding_dim, attention_type, dropout_rate))\n",
        "\n",
        "        # CRF Layer \n",
        "        self.crf_layer = CRF_layer(tag_to_ix)\n",
        "\n",
        "        # Maps the output of the LSTM into tag space.\n",
        "        self.hidden2tag = nn.Linear(hidden_dim, self.tagset_size)\n",
        "\n",
        "    # Forward pass without CRF layer\n",
        "    def _forward_pass(self, sentence):\n",
        "        # Word Embeddings\n",
        "        word_embeds = self.word_embeds(sentence).view(len(sentence), 1, -1)\n",
        "\n",
        "        # PoS Tag Embeddings\n",
        "        # pos_embeds = self.pos_embeds(sentence).view(len(sentence), 1, -1)\n",
        "        # pos_embeds = self.pos_linear(pos_embeds)\n",
        "\n",
        "        # Concated Embeddings\n",
        "        # embeds = torch.cat((word_embeds, pos_embeds), 2)\n",
        "\n",
        "        # LSTM Layers\n",
        "        lstm_out, _ = self.lstm(word_embeds)\n",
        "\n",
        "        # Attention Calculation\n",
        "        attention_score = lstm_out\n",
        "        for layer in self.attention_layers[:-1]:\n",
        "            attention_score = layer(attention_score)\n",
        "\n",
        "        att_out = lstm_out.view(len(sentence), self.hidden_dim)\n",
        "        feats = self.hidden2tag(att_out)\n",
        "\n",
        "        return feats\n",
        "\n",
        "    # Use where Forward Propogation during training\n",
        "    def forward_training(self, sentence, tags):\n",
        "        feats = self._forward_pass(sentence)\n",
        "        forward_score = self.crf_layer.forward(feats)\n",
        "\n",
        "        # gold_score = self._score_sentence(feats, tags)\n",
        "        gold_score = self.crf_layer._score_sentence(feats, tags)\n",
        "\n",
        "        return forward_score - gold_score\n",
        "\n",
        "    # Use for inference\n",
        "    def forward(self, sentence): \n",
        "        # Get the emission scores from the BiLSTM\n",
        "        feats = self._forward_pass(sentence)\n",
        "\n",
        "        # Find the best path, given the features.\n",
        "        score, tag_seq = self.crf_layer.decode(feats)\n",
        "        return score, tag_seq\n",
        "# Calculate the scores given ground truth and prediction\n",
        "def cal_acc(model, input_index, output_index):\n",
        "    ground_truth = []\n",
        "    predicted = []\n",
        "    for i,idxs in enumerate(input_index):\n",
        "        ground_truth += output_index[i]\n",
        "        score, pred = model(torch.tensor(idxs, dtype=torch.long).to(device))\n",
        "        predicted += pred\n",
        "    accuracy = sum(np.array(ground_truth) == np.array(predicted))/len(ground_truth)\n",
        "    f1 = f1_score(ground_truth, predicted, average='weighted')\n",
        "    return predicted, ground_truth, accuracy, f1\n",
        "\n",
        "\n",
        "\n",
        "# Map the tag index to NER tag name\n",
        "def decode_output(output_list):\n",
        "    ix_to_tag = {v:k for k,v in tag_to_ix.items()}\n",
        "    return [ix_to_tag[output] for output in output_list]\n",
        "\n",
        "\n",
        "# Print out the prediction sample and classification report\n",
        "def evaluate_model(model):\n",
        "    # Get the results\n",
        "    y_pred, y_true, _, _ = cal_acc(model, val_input_index, val_output_index)\n",
        "    y_true_decode = decode_output(y_true)\n",
        "    y_pred_decode = decode_output(y_pred)\n",
        "\n",
        "    print(f'Sample ground truth: {y_true_decode[:20]}')\n",
        "    print(f'Sample prediction: {y_pred_decode[:20]}')\n",
        "\n",
        "    print(classification_report(y_true_decode, y_pred_decode, digits=4))\n",
        "\n",
        "\n",
        "def define_train_evaluate_model(model, num_epoch=10):\n",
        "    # For recording the training log for visualisation\n",
        "    log = {\n",
        "        'train_loss': [],\n",
        "        'train_acc': [],\n",
        "        'train_f1': [],\n",
        "        'val_loss': [],\n",
        "        'val_acc': [],\n",
        "        'val_f1': [],\n",
        "    }\n",
        "\n",
        "    # Training Loop\n",
        "    for epoch in range(num_epoch):  \n",
        "        time1 = datetime.datetime.now()\n",
        "        train_loss = 0\n",
        "\n",
        "        model.train()\n",
        "        # Mini-Batch Training\n",
        "        for i, idxs in enumerate(train_input_index):\n",
        "            tags_index = train_output_index[i]\n",
        "\n",
        "            # Clear the gradients\n",
        "            model.zero_grad()\n",
        "\n",
        "            # Prepare the input and output data as tensors \n",
        "            sentence_in = torch.tensor(idxs, dtype=torch.long).to(device)\n",
        "            targets = torch.tensor(tags_index, dtype=torch.long).to(device)\n",
        "\n",
        "            # Forward propogation\n",
        "            loss = model.forward_training(sentence_in, targets)\n",
        "\n",
        "            # Backward Propogation\n",
        "            loss.backward()\n",
        "\n",
        "            # Update the model\n",
        "            optimizer.step()\n",
        "\n",
        "            # Accumulate the loss\n",
        "            train_loss+=loss.item()\n",
        "\n",
        "        # Evaluate the model on train_set and val_set\n",
        "        model.eval()\n",
        "        _, _, train_acc, train_f1 = cal_acc(model,train_input_index,train_output_index)\n",
        "        _, _, val_acc, val_f1 = cal_acc(model,val_input_index,val_output_index)\n",
        "\n",
        "        # Evaluate on the validation dataset \n",
        "        val_loss = 0\n",
        "        for i, idxs in enumerate(val_input_index):\n",
        "            tags_index = val_output_index[i]\n",
        "            sentence_in = torch.tensor(idxs, dtype=torch.long).to(device)\n",
        "            targets = torch.tensor(tags_index, dtype=torch.long).to(device)\n",
        "            loss = model.forward_training(sentence_in, targets)\n",
        "            val_loss+=loss.item()\n",
        "        time2 = datetime.datetime.now()\n",
        "\n",
        "        # The log of each epoch\n",
        "        print(\"Epoch:%d, Training loss: %.2f, train acc: %.4f, train f1: %.4f, val loss: %.2f, val acc: %.4f, val f1: %.4f, time: %.2fs\" \\\n",
        "            %(epoch+1, train_loss, train_acc, train_f1, val_loss, val_acc, val_f1, (time2-time1).total_seconds()))\n",
        "        # Log the training log of each epoch\n",
        "        log['train_loss'].append(train_loss)\n",
        "        log['train_acc'].append(train_acc)\n",
        "        log['train_f1'].append(val_acc)\n",
        "        log['val_loss'].append(val_loss)\n",
        "        log['val_acc'].append(val_acc)\n",
        "        log['val_f1'].append(val_acc)\n",
        "\n",
        "    # Evaluate the model\n",
        "    evaluate_model(model)\n",
        "\n",
        "    return model, log\n",
        "# LSTM\n",
        "HIDDEN_DIM = 50                # the number of hidden dimension on LSTM\n",
        "HIDDEN_LAYER = 1                # the number of hidden layer of LSTM\n",
        "\n",
        "# Attention\n",
        "ATTENTION_LAYER = 1             # the number of attention layer\n",
        "ATTENTION_TYPE = 'cos_sim'   # dot | scaled_dot | cos_sim\n",
        "\n",
        "# Dropout rate on LSTM layers and Attention units\n",
        "DROPOUT_RATE = 0.0\n",
        "\n",
        "# Training\n",
        "NUM_EPOCH = 2\n",
        "LR = 0.001\n",
        "WEIGHT_DECAY = 1e-4\n",
        "\n",
        "# Create the model \n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model_att_cosm = Model(tag_to_ix, EMBEDDING_DIM, HIDDEN_DIM, HIDDEN_LAYER, DROPOUT_RATE, ATTENTION_LAYER, ATTENTION_TYPE).to(device)\n",
        "optimizer = optim.SGD(model_att_cosm.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)\n",
        "print(f'Model Setup: \\n{model_att_cosm}\\n')\n",
        "print(f'Optimizer Setup: \\n{optimizer}\\n')\n",
        "print(f'------ Start Training, Testing, Evaluating Process | Total Epoch: {NUM_EPOCH} ------')\n",
        "\n",
        "# Start Training, testing, evaluating process\n",
        "model_att_cosm, log = define_train_evaluate_model(model_att_cosm,\n",
        "                         num_epoch=NUM_EPOCH)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "86bEDgF5H6uD",
        "outputId": "93a056fd-8ea4-4ac8-cddb-dcd5416d9c29"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Setup: \n",
            "Model(\n",
            "  (word_embeds): Embedding(11243, 25)\n",
            "  (lstm): LSTM(25, 25, bidirectional=True)\n",
            "  (attention_layers): ModuleList(\n",
            "    (0): Attention_layer(\n",
            "      (K): Linear(in_features=50, out_features=50, bias=False)\n",
            "      (Q): Linear(in_features=50, out_features=50, bias=False)\n",
            "      (V): Linear(in_features=50, out_features=50, bias=False)\n",
            "      (K_dropout): Dropout(p=0.0, inplace=False)\n",
            "      (Q_dropout): Dropout(p=0.0, inplace=False)\n",
            "      (V_dropout): Dropout(p=0.0, inplace=False)\n",
            "    )\n",
            "  )\n",
            "  (crf_layer): CRF_layer()\n",
            "  (hidden2tag): Linear(in_features=50, out_features=12, bias=True)\n",
            ")\n",
            "\n",
            "Optimizer Setup: \n",
            "SGD (\n",
            "Parameter Group 0\n",
            "    dampening: 0\n",
            "    lr: 0.001\n",
            "    maximize: False\n",
            "    momentum: 0\n",
            "    nesterov: False\n",
            "    weight_decay: 0.0001\n",
            ")\n",
            "\n",
            "------ Start Training, Testing, Evaluating Process | Total Epoch: 2 ------\n",
            "Epoch:1, Training loss: 119679.53, train acc: 0.6750, train f1: 0.5440, val loss: 35794.57, val acc: 0.6772, val f1: 0.5469, time: 697.85s\n",
            "Epoch:2, Training loss: 93960.33, train acc: 0.8042, train f1: 0.7474, val loss: 22791.76, val acc: 0.8034, val f1: 0.7462, time: 689.89s\n",
            "Sample ground truth: ['s', 's', 's', 'o', 'o', 'o', 'o', 't', 'o', 'o', 'p', 'o', 'o', 'p', 'o', 't', 'o', 't', 'c', 'o']\n",
            "Sample prediction: ['s', 's', 'p', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'p', 'o', 'o', 'p', 'o', 'o', 'o', 'o', 'o', 'o']\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           c     0.0000    0.0000    0.0000      1641\n",
            "           d     0.0000    0.0000    0.0000       398\n",
            "           o     0.8136    1.0000    0.8972     22588\n",
            "           p     0.7276    0.7533    0.7402      3936\n",
            "           s     0.8225    0.3739    0.5141      3322\n",
            "           t     0.0000    0.0000    0.0000      1469\n",
            "\n",
            "    accuracy                         0.8034     33354\n",
            "   macro avg     0.3940    0.3545    0.3586     33354\n",
            "weighted avg     0.7188    0.8034    0.7462     33354\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Model(Pos Tagging Embedding + Word2Vec + Bi-LSTM + CRF + Attention(Scaled Dot))"
      ],
      "metadata": {
        "id": "rVy7jnkCKr-7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Model(nn.Module):\n",
        "    def __init__(self, tag_to_ix, embedding_dim, hidden_dim, hidden_layer, dropout_rate, attention_layer, attention_type):\n",
        "        super(Model, self).__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.tagset_size = len(tag_to_ix)\n",
        "        self.attention_layer = attention_layer\n",
        "\n",
        "        # Word Embeddings \n",
        "        self.word_embeds = nn.Embedding(VOCAB_SIZE, embedding_dim)\n",
        "        self.word_embeds.weight.data.copy_(torch.from_numpy(embedding_matrix))\n",
        "\n",
        "        # Pos Embeddings\n",
        "        self.pos_embeds = nn.Embedding(VOCAB_SIZE, POSTAG_SIZE)\n",
        "        self.pos_embeds.weight.data.copy_(torch.from_numpy(pos_embedding_matrix))\n",
        "        self.pos_linear = nn.Linear(POSTAG_SIZE, POSTAG_SIZE)\n",
        "\n",
        "        # Bi-LSTM Layers\n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_dim // 2, dropout=dropout_rate,\n",
        "                            num_layers=hidden_layer, bidirectional=True)\n",
        "\n",
        "        # Attention Layers\n",
        "        self.attention_layers = nn.ModuleList()\n",
        "        for layer in range(attention_layer):\n",
        "            self.attention_layers.append(Attention_layer(embedding_dim+embedding_dim, attention_type, dropout_rate))\n",
        "\n",
        "        # CRF Layer \n",
        "        self.crf_layer = CRF_layer(tag_to_ix)\n",
        "\n",
        "        # Maps the output of the LSTM into tag space.\n",
        "        self.hidden2tag = nn.Linear(hidden_dim, self.tagset_size)\n",
        "\n",
        "    # Forward pass without CRF layer\n",
        "    def _forward_pass(self, sentence):\n",
        "        # Word Embeddings\n",
        "        word_embeds = self.word_embeds(sentence).view(len(sentence), 1, -1)\n",
        "\n",
        "        # PoS Tag Embeddings\n",
        "        pos_embeds = self.pos_embeds(sentence).view(len(sentence), 1, -1)\n",
        "        pos_embeds = self.pos_linear(pos_embeds)\n",
        "\n",
        "        # Concated Embeddings\n",
        "        embeds = torch.cat((word_embeds, pos_embeds), 2)\n",
        "\n",
        "        # LSTM Layers\n",
        "        lstm_out, _ = self.lstm(word_embeds)\n",
        "\n",
        "        # Attention Calculation\n",
        "        attention_score = lstm_out\n",
        "        for layer in self.attention_layers[:-1]:\n",
        "            attention_score = layer(attention_score)\n",
        "\n",
        "        att_out = lstm_out.view(len(sentence), self.hidden_dim)\n",
        "        feats = self.hidden2tag(att_out)\n",
        "\n",
        "        return feats\n",
        "\n",
        "    # Use where Forward Propogation during training\n",
        "    def forward_training(self, sentence, tags):\n",
        "        feats = self._forward_pass(sentence)\n",
        "        forward_score = self.crf_layer.forward(feats)\n",
        "\n",
        "        # gold_score = self._score_sentence(feats, tags)\n",
        "        gold_score = self.crf_layer._score_sentence(feats, tags)\n",
        "\n",
        "        return forward_score - gold_score\n",
        "\n",
        "    # Use for inference\n",
        "    def forward(self, sentence): \n",
        "        # Get the emission scores from the BiLSTM\n",
        "        feats = self._forward_pass(sentence)\n",
        "\n",
        "        # Find the best path, given the features.\n",
        "        score, tag_seq = self.crf_layer.decode(feats)\n",
        "        return score, tag_seq\n",
        "# Calculate the scores given ground truth and prediction\n",
        "def cal_acc(model, input_index, output_index):\n",
        "    ground_truth = []\n",
        "    predicted = []\n",
        "    for i,idxs in enumerate(input_index):\n",
        "        ground_truth += output_index[i]\n",
        "        score, pred = model(torch.tensor(idxs, dtype=torch.long).to(device))\n",
        "        predicted += pred\n",
        "    accuracy = sum(np.array(ground_truth) == np.array(predicted))/len(ground_truth)\n",
        "    f1 = f1_score(ground_truth, predicted, average='weighted')\n",
        "    return predicted, ground_truth, accuracy, f1\n",
        "\n",
        "\n",
        "\n",
        "# Map the tag index to NER tag name\n",
        "def decode_output(output_list):\n",
        "    ix_to_tag = {v:k for k,v in tag_to_ix.items()}\n",
        "    return [ix_to_tag[output] for output in output_list]\n",
        "\n",
        "\n",
        "# Print out the prediction sample and classification report\n",
        "def evaluate_model(model):\n",
        "    # Get the results\n",
        "    y_pred, y_true, _, _ = cal_acc(model, val_input_index, val_output_index)\n",
        "    y_true_decode = decode_output(y_true)\n",
        "    y_pred_decode = decode_output(y_pred)\n",
        "\n",
        "    print(f'Sample ground truth: {y_true_decode[:20]}')\n",
        "    print(f'Sample prediction: {y_pred_decode[:20]}')\n",
        "\n",
        "    print(classification_report(y_true_decode, y_pred_decode, digits=4))\n",
        "\n",
        "\n",
        "def define_train_evaluate_model(model, num_epoch=10):\n",
        "    # For recording the training log for visualisation\n",
        "    log = {\n",
        "        'train_loss': [],\n",
        "        'train_acc': [],\n",
        "        'train_f1': [],\n",
        "        'val_loss': [],\n",
        "        'val_acc': [],\n",
        "        'val_f1': [],\n",
        "    }\n",
        "\n",
        "    # Training Loop\n",
        "    for epoch in range(num_epoch):  \n",
        "        time1 = datetime.datetime.now()\n",
        "        train_loss = 0\n",
        "\n",
        "        model.train()\n",
        "        # Mini-Batch Training\n",
        "        for i, idxs in enumerate(train_input_index):\n",
        "            tags_index = train_output_index[i]\n",
        "\n",
        "            # Clear the gradients\n",
        "            model.zero_grad()\n",
        "\n",
        "            # Prepare the input and output data as tensors \n",
        "            sentence_in = torch.tensor(idxs, dtype=torch.long).to(device)\n",
        "            targets = torch.tensor(tags_index, dtype=torch.long).to(device)\n",
        "\n",
        "            # Forward propogation\n",
        "            loss = model.forward_training(sentence_in, targets)\n",
        "\n",
        "            # Backward Propogation\n",
        "            loss.backward()\n",
        "\n",
        "            # Update the model\n",
        "            optimizer.step()\n",
        "\n",
        "            # Accumulate the loss\n",
        "            train_loss+=loss.item()\n",
        "\n",
        "        # Evaluate the model on train_set and val_set\n",
        "        model.eval()\n",
        "        _, _, train_acc, train_f1 = cal_acc(model,train_input_index,train_output_index)\n",
        "        _, _, val_acc, val_f1 = cal_acc(model,val_input_index,val_output_index)\n",
        "\n",
        "        # Evaluate on the validation dataset \n",
        "        val_loss = 0\n",
        "        for i, idxs in enumerate(val_input_index):\n",
        "            tags_index = val_output_index[i]\n",
        "            sentence_in = torch.tensor(idxs, dtype=torch.long).to(device)\n",
        "            targets = torch.tensor(tags_index, dtype=torch.long).to(device)\n",
        "            loss = model.forward_training(sentence_in, targets)\n",
        "            val_loss+=loss.item()\n",
        "        time2 = datetime.datetime.now()\n",
        "\n",
        "        # The log of each epoch\n",
        "        print(\"Epoch:%d, Training loss: %.2f, train acc: %.4f, train f1: %.4f, val loss: %.2f, val acc: %.4f, val f1: %.4f, time: %.2fs\" \\\n",
        "            %(epoch+1, train_loss, train_acc, train_f1, val_loss, val_acc, val_f1, (time2-time1).total_seconds()))\n",
        "        # Log the training log of each epoch\n",
        "        log['train_loss'].append(train_loss)\n",
        "        log['train_acc'].append(train_acc)\n",
        "        log['train_f1'].append(val_acc)\n",
        "        log['val_loss'].append(val_loss)\n",
        "        log['val_acc'].append(val_acc)\n",
        "        log['val_f1'].append(val_acc)\n",
        "\n",
        "    # Evaluate the model\n",
        "    evaluate_model(model)\n",
        "\n",
        "    return model, log\n",
        "# LSTM\n",
        "HIDDEN_DIM = 50                # the number of hidden dimension on LSTM\n",
        "HIDDEN_LAYER = 1                # the number of hidden layer of LSTM\n",
        "\n",
        "# Attention\n",
        "ATTENTION_LAYER = 1             # the number of attention layer\n",
        "ATTENTION_TYPE = 'scaled_dot'   # dot | scaled_dot | cos_sim\n",
        "\n",
        "# Dropout rate on LSTM layers and Attention units\n",
        "DROPOUT_RATE = 0.0\n",
        "\n",
        "# Training\n",
        "NUM_EPOCH = 2\n",
        "LR = 0.001\n",
        "WEIGHT_DECAY = 1e-4\n",
        "\n",
        "# Create the model \n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model_attn_scaledot = Model(tag_to_ix, EMBEDDING_DIM, HIDDEN_DIM, HIDDEN_LAYER, DROPOUT_RATE, ATTENTION_LAYER, ATTENTION_TYPE).to(device)\n",
        "optimizer = optim.SGD(model_attn_scaledot.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)\n",
        "print(f'Model Setup: \\n{model_attn_scaledot}\\n')\n",
        "print(f'Optimizer Setup: \\n{optimizer}\\n')\n",
        "print(f'------ Start Training, Testing, Evaluating Process | Total Epoch: {NUM_EPOCH} ------')\n",
        "\n",
        "# Start Training, testing, evaluating process\n",
        "model_attn_scaledot, log = define_train_evaluate_model(model_attn_scaledot,\n",
        "                         num_epoch=NUM_EPOCH)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GfL3nq6IKtXf",
        "outputId": "a8d87ce4-3b51-44a6-9d7e-ab06692dc8e9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Setup: \n",
            "Model(\n",
            "  (word_embeds): Embedding(11243, 25)\n",
            "  (pos_embeds): Embedding(11243, 49)\n",
            "  (pos_linear): Linear(in_features=49, out_features=49, bias=True)\n",
            "  (lstm): LSTM(25, 25, bidirectional=True)\n",
            "  (attention_layers): ModuleList(\n",
            "    (0): Attention_layer(\n",
            "      (K): Linear(in_features=50, out_features=50, bias=False)\n",
            "      (Q): Linear(in_features=50, out_features=50, bias=False)\n",
            "      (V): Linear(in_features=50, out_features=50, bias=False)\n",
            "      (K_dropout): Dropout(p=0.0, inplace=False)\n",
            "      (Q_dropout): Dropout(p=0.0, inplace=False)\n",
            "      (V_dropout): Dropout(p=0.0, inplace=False)\n",
            "    )\n",
            "  )\n",
            "  (crf_layer): CRF_layer()\n",
            "  (hidden2tag): Linear(in_features=50, out_features=12, bias=True)\n",
            ")\n",
            "\n",
            "Optimizer Setup: \n",
            "SGD (\n",
            "Parameter Group 0\n",
            "    dampening: 0\n",
            "    lr: 0.001\n",
            "    maximize: False\n",
            "    momentum: 0\n",
            "    nesterov: False\n",
            "    weight_decay: 0.0001\n",
            ")\n",
            "\n",
            "------ Start Training, Testing, Evaluating Process | Total Epoch: 2 ------\n",
            "Epoch:1, Training loss: 114532.29, train acc: 0.6750, train f1: 0.5440, val loss: 35252.63, val acc: 0.6772, val f1: 0.5469, time: 698.43s\n",
            "Epoch:2, Training loss: 98847.74, train acc: 0.7713, train f1: 0.7073, val loss: 26637.26, val acc: 0.7715, val f1: 0.7071, time: 695.21s\n",
            "Sample ground truth: ['s', 's', 's', 'o', 'o', 'o', 'o', 't', 'o', 'o', 'p', 'o', 'o', 'p', 'o', 't', 'o', 't', 'c', 'o']\n",
            "Sample prediction: ['s', 's', 'p', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'p', 'o', 'o', 'p', 'o', 'o', 'o', 'o', 'o', 'o']\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           c     0.0000    0.0000    0.0000      1641\n",
            "           d     0.0000    0.0000    0.0000       398\n",
            "           o     0.7721    1.0000    0.8714     22588\n",
            "           p     0.7370    0.5681    0.6416      3936\n",
            "           s     0.8518    0.2733    0.4139      3322\n",
            "           t     0.0000    0.0000    0.0000      1469\n",
            "\n",
            "    accuracy                         0.7715     33354\n",
            "   macro avg     0.3935    0.3069    0.3211     33354\n",
            "weighted avg     0.6947    0.7715    0.7071     33354\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Model(Pos + Dependency + Word Embedding + Bi-LSTM + CRF + Attention(Scaled Dot))"
      ],
      "metadata": {
        "id": "SPMGPwuwKnWE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Model(nn.Module):\n",
        "    def __init__(self, tag_to_ix, embedding_dim, hidden_dim, hidden_layer, dropout_rate, attention_layer, attention_type):\n",
        "        super(Model, self).__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.tagset_size = len(tag_to_ix)\n",
        "        self.attention_layer = attention_layer\n",
        "\n",
        "        # Word Embeddings \n",
        "        self.word_embeds = nn.Embedding(VOCAB_SIZE, embedding_dim)\n",
        "        self.word_embeds.weight.data.copy_(torch.from_numpy(embedding_matrix))\n",
        "\n",
        "        # Pos Embeddings\n",
        "        self.pos_embeds = nn.Embedding(VOCAB_SIZE, POSTAG_SIZE)\n",
        "        self.pos_embeds.weight.data.copy_(torch.from_numpy(pos_embedding_matrix))\n",
        "        self.pos_linear = nn.Linear(POSTAG_SIZE, POSTAG_SIZE)\n",
        "\n",
        "        # Bi-LSTM Layers\n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_dim // 2, dropout=dropout_rate,\n",
        "                            num_layers=hidden_layer, bidirectional=True)\n",
        "\n",
        "        # Attention Layers\n",
        "        self.attention_layers = nn.ModuleList()\n",
        "        for layer in range(attention_layer):\n",
        "            self.attention_layers.append(Attention_layer(embedding_dim+embedding_dim, attention_type, dropout_rate))\n",
        "\n",
        "        # CRF Layer \n",
        "        self.crf_layer = CRF_layer(tag_to_ix)\n",
        "\n",
        "        # Maps the output of the LSTM into tag space.\n",
        "        self.hidden2tag = nn.Linear(hidden_dim, self.tagset_size)\n",
        "\n",
        "    # Forward pass without CRF layer\n",
        "    def _forward_pass(self, sentence):\n",
        "        # Word Embeddings\n",
        "        word_embeds = self.word_embeds(sentence).view(len(sentence), 1, -1)\n",
        "\n",
        "        # PoS Tag Embeddings\n",
        "        pos_embeds = self.pos_embeds(sentence).view(len(sentence), 1, -1)\n",
        "        pos_embeds = self.pos_linear(pos_embeds)\n",
        "\n",
        "        # Concated Embeddings\n",
        "        embeds = torch.cat((word_embeds, pos_embeds), 2)\n",
        "\n",
        "        # LSTM Layers\n",
        "        lstm_out, _ = self.lstm(word_embeds)\n",
        "\n",
        "        # Attention Calculation\n",
        "        attention_score = lstm_out\n",
        "        for layer in self.attention_layers[:-1]:\n",
        "            attention_score = layer(attention_score)\n",
        "\n",
        "        att_out = lstm_out.view(len(sentence), self.hidden_dim)\n",
        "        feats = self.hidden2tag(att_out)\n",
        "\n",
        "        return feats\n",
        "\n",
        "    # Use where Forward Propogation during training\n",
        "    def forward_training(self, sentence, tags):\n",
        "        feats = self._forward_pass(sentence)\n",
        "        forward_score = self.crf_layer.forward(feats)\n",
        "\n",
        "        # gold_score = self._score_sentence(feats, tags)\n",
        "        gold_score = self.crf_layer._score_sentence(feats, tags)\n",
        "\n",
        "        return forward_score - gold_score\n",
        "\n",
        "    # Use for inference\n",
        "    def forward(self, sentence): \n",
        "        # Get the emission scores from the BiLSTM\n",
        "        feats = self._forward_pass(sentence)\n",
        "\n",
        "        # Find the best path, given the features.\n",
        "        score, tag_seq = self.crf_layer.decode(feats)\n",
        "        return score, tag_seq\n",
        "# Calculate the scores given ground truth and prediction\n",
        "def cal_acc(model, input_index, output_index):\n",
        "    ground_truth = []\n",
        "    predicted = []\n",
        "    for i,idxs in enumerate(input_index):\n",
        "        ground_truth += output_index[i]\n",
        "        score, pred = model(torch.tensor(idxs, dtype=torch.long).to(device))\n",
        "        predicted += pred\n",
        "    accuracy = sum(np.array(ground_truth) == np.array(predicted))/len(ground_truth)\n",
        "    f1 = f1_score(ground_truth, predicted, average='weighted')\n",
        "    return predicted, ground_truth, accuracy, f1\n",
        "\n",
        "\n",
        "\n",
        "# Map the tag index to NER tag name\n",
        "def decode_output(output_list):\n",
        "    ix_to_tag = {v:k for k,v in tag_to_ix.items()}\n",
        "    return [ix_to_tag[output] for output in output_list]\n",
        "\n",
        "\n",
        "# Print out the prediction sample and classification report\n",
        "def evaluate_model(model):\n",
        "    # Get the results\n",
        "    y_pred, y_true, _, _ = cal_acc(model, val_input_index, val_output_index)\n",
        "    y_true_decode = decode_output(y_true)\n",
        "    y_pred_decode = decode_output(y_pred)\n",
        "\n",
        "    print(f'Sample ground truth: {y_true_decode[:20]}')\n",
        "    print(f'Sample prediction: {y_pred_decode[:20]}')\n",
        "\n",
        "    print(classification_report(y_true_decode, y_pred_decode, digits=4))\n",
        "\n",
        "\n",
        "def define_train_evaluate_model(model, num_epoch=10):\n",
        "    # For recording the training log for visualisation\n",
        "    log = {\n",
        "        'train_loss': [],\n",
        "        'train_acc': [],\n",
        "        'train_f1': [],\n",
        "        'val_loss': [],\n",
        "        'val_acc': [],\n",
        "        'val_f1': [],\n",
        "    }\n",
        "\n",
        "    # Training Loop\n",
        "    for epoch in range(num_epoch):  \n",
        "        time1 = datetime.datetime.now()\n",
        "        train_loss = 0\n",
        "\n",
        "        model.train()\n",
        "        # Mini-Batch Training\n",
        "        for i, idxs in enumerate(train_input_index):\n",
        "            tags_index = train_output_index[i]\n",
        "\n",
        "            # Clear the gradients\n",
        "            model.zero_grad()\n",
        "\n",
        "            # Prepare the input and output data as tensors \n",
        "            sentence_in = torch.tensor(idxs, dtype=torch.long).to(device)\n",
        "            targets = torch.tensor(tags_index, dtype=torch.long).to(device)\n",
        "\n",
        "            # Forward propogation\n",
        "            loss = model.forward_training(sentence_in, targets)\n",
        "\n",
        "            # Backward Propogation\n",
        "            loss.backward()\n",
        "\n",
        "            # Update the model\n",
        "            optimizer.step()\n",
        "\n",
        "            # Accumulate the loss\n",
        "            train_loss+=loss.item()\n",
        "\n",
        "        # Evaluate the model on train_set and val_set\n",
        "        model.eval()\n",
        "        _, _, train_acc, train_f1 = cal_acc(model,train_input_index,train_output_index)\n",
        "        _, _, val_acc, val_f1 = cal_acc(model,val_input_index,val_output_index)\n",
        "\n",
        "        # Evaluate on the validation dataset \n",
        "        val_loss = 0\n",
        "        for i, idxs in enumerate(val_input_index):\n",
        "            tags_index = val_output_index[i]\n",
        "            sentence_in = torch.tensor(idxs, dtype=torch.long).to(device)\n",
        "            targets = torch.tensor(tags_index, dtype=torch.long).to(device)\n",
        "            loss = model.forward_training(sentence_in, targets)\n",
        "            val_loss+=loss.item()\n",
        "        time2 = datetime.datetime.now()\n",
        "\n",
        "        # The log of each epoch\n",
        "        print(\"Epoch:%d, Training loss: %.2f, train acc: %.4f, train f1: %.4f, val loss: %.2f, val acc: %.4f, val f1: %.4f, time: %.2fs\" \\\n",
        "            %(epoch+1, train_loss, train_acc, train_f1, val_loss, val_acc, val_f1, (time2-time1).total_seconds()))\n",
        "        # Log the training log of each epoch\n",
        "        log['train_loss'].append(train_loss)\n",
        "        log['train_acc'].append(train_acc)\n",
        "        log['train_f1'].append(val_acc)\n",
        "        log['val_loss'].append(val_loss)\n",
        "        log['val_acc'].append(val_acc)\n",
        "        log['val_f1'].append(val_acc)\n",
        "\n",
        "    # Evaluate the model\n",
        "    evaluate_model(model)\n",
        "\n",
        "    return model, log\n",
        "# LSTM\n",
        "HIDDEN_DIM = 50                # the number of hidden dimension on LSTM\n",
        "HIDDEN_LAYER = 1                # the number of hidden layer of LSTM\n",
        "\n",
        "# Attention\n",
        "ATTENTION_LAYER = 1             # the number of attention layer\n",
        "ATTENTION_TYPE = 'scaled_dot'   # dot | scaled_dot | cos_sim\n",
        "\n",
        "# Dropout rate on LSTM layers and Attention units\n",
        "DROPOUT_RATE = 0.0\n",
        "\n",
        "# Training\n",
        "NUM_EPOCH = 2\n",
        "LR = 0.001\n",
        "WEIGHT_DECAY = 1e-4\n",
        "\n",
        "# Create the model \n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model_attn_scaledot = Model(tag_to_ix, EMBEDDING_DIM, HIDDEN_DIM, HIDDEN_LAYER, DROPOUT_RATE, ATTENTION_LAYER, ATTENTION_TYPE).to(device)\n",
        "optimizer = optim.SGD(model_attn_scaledot.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)\n",
        "print(f'Model Setup: \\n{model_attn_scaledot}\\n')\n",
        "print(f'Optimizer Setup: \\n{optimizer}\\n')\n",
        "print(f'------ Start Training, Testing, Evaluating Process | Total Epoch: {NUM_EPOCH} ------')\n",
        "\n",
        "# Start Training, testing, evaluating process\n",
        "model_attn_scaledot, log = define_train_evaluate_model(model_attn_scaledot,\n",
        "                         num_epoch=NUM_EPOCH)"
      ],
      "metadata": {
        "id": "Nw1pJEroKTUt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Model(Word2Vec + Bi-LSTM + Attention(Scaled Dot) + Without CRF)"
      ],
      "metadata": {
        "id": "XsboGoh9yAx6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Model(nn.Module):\n",
        "    def __init__(self, tag_to_ix, embedding_dim, hidden_dim, hidden_layer, dropout_rate, attention_layer, attention_type):\n",
        "        super(Model, self).__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.tagset_size = len(tag_to_ix)\n",
        "        self.attention_layer = attention_layer\n",
        "\n",
        "        # Word Embeddings \n",
        "        self.word_embeds = nn.Embedding(VOCAB_SIZE, embedding_dim)\n",
        "        self.word_embeds.weight.data.copy_(torch.from_numpy(embedding_matrix))\n",
        "\n",
        "        # Pos Embeddings\n",
        "        # self.pos_embeds = nn.Embedding(VOCAB_SIZE, POSTAG_SIZE)\n",
        "        # self.pos_embeds.weight.data.copy_(torch.from_numpy(pos_embedding_matrix))\n",
        "        # self.pos_linear = nn.Linear(POSTAG_SIZE, POSTAG_SIZE)\n",
        "\n",
        "        # Bi-LSTM Layers\n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_dim // 2, dropout=dropout_rate,\n",
        "                            num_layers=hidden_layer, bidirectional=True)\n",
        "\n",
        "        # Attention Layers\n",
        "        self.attention_layers = nn.ModuleList()\n",
        "        for layer in range(attention_layer):\n",
        "            self.attention_layers.append(Attention_layer(embedding_dim+embedding_dim, attention_type, dropout_rate))\n",
        "\n",
        "        # CRF Layer \n",
        "        # self.crf_layer = CRF_layer(tag_to_ix)\n",
        "\n",
        "        # Maps the output of the LSTM into tag space.\n",
        "        self.hidden2tag = nn.Linear(hidden_dim, self.tagset_size)\n",
        "\n",
        "        self.ce = nn.CrossEntropyLoss()\n",
        "\n",
        "    # Forward pass without CRF layer\n",
        "    def _forward_pass(self, sentence):\n",
        "        # Word Embeddings\n",
        "        word_embeds = self.word_embeds(sentence).view(len(sentence), 1, -1)\n",
        "\n",
        "        # PoS Tag Embeddings\n",
        "        # pos_embeds = self.pos_embeds(sentence).view(len(sentence), 1, -1)\n",
        "        # pos_embeds = self.pos_linear(pos_embeds)\n",
        "\n",
        "        # Concated Embeddings\n",
        "        # embeds = torch.cat((word_embeds, pos_embeds), 2)\n",
        "\n",
        "        # LSTM Layers\n",
        "        lstm_out, _ = self.lstm(word_embeds)\n",
        "\n",
        "        # Attention Calculation\n",
        "        attention_score = lstm_out\n",
        "        for layer in self.attention_layers[:-1]:\n",
        "            attention_score = layer(attention_score)\n",
        "\n",
        "        att_out = lstm_out.view(len(sentence), self.hidden_dim)\n",
        "        feats = self.hidden2tag(att_out)\n",
        "\n",
        "        return feats\n",
        "\n",
        "    # Use where Forward Propogation during training\n",
        "    def forward_training(self, sentence, tags):\n",
        "        feats = self._forward_pass(sentence)\n",
        "        tag_seq = torch.argmax(feats, dim=1)\n",
        "        score = self.ce(feats, tags)\n",
        "\n",
        "        return score\n",
        "\n",
        "    # Use for inference\n",
        "    def forward(self, sentence): \n",
        "        # Get the emission scores from the BiLSTM\n",
        "        feats = self._forward_pass(sentence)\n",
        "        tag_seq = torch.argmax(feats, dim=1)\n",
        "        score = self.ce(feats, tag_seq)\n",
        "\n",
        "        return score, tag_seq\n",
        "# Calculate the scores given ground truth and prediction\n",
        "def cal_acc(model, input_index, output_index):\n",
        "    ground_truth = []\n",
        "    predicted = []\n",
        "    for i,idxs in enumerate(input_index):\n",
        "        ground_truth += output_index[i]\n",
        "        score, pred = model(torch.tensor(idxs, dtype=torch.long).to(device))\n",
        "        # print(pred)\n",
        "        # print(pred.numpy())\n",
        "        predicted += pred.cpu()\n",
        "    accuracy = sum(np.array(ground_truth) == np.array(predicted))/len(ground_truth)\n",
        "    f1 = f1_score(ground_truth, predicted, average='weighted')\n",
        "    return predicted, ground_truth, accuracy, f1\n",
        "\n",
        "\n",
        "# Map the tag index to NER tag name\n",
        "def decode_output(output_list):\n",
        "    ix_to_tag = {v:k for k,v in tag_to_ix.items()}\n",
        "    return [ix_to_tag[output] for output in output_list]\n",
        "\n",
        "\n",
        "# Print out the prediction sample and classification report\n",
        "def evaluate_model(model):\n",
        "    # Get the results\n",
        "    y_pred, y_true, _, _ = cal_acc(model, val_input_index, val_output_index)\n",
        "    y_true_decode = decode_output(y_true)\n",
        "    y_pred_decode = decode_output(list(y_pred))\n",
        "    print(f'Sample ground truth: {y_true_decode[:20]}')\n",
        "    print(f'Sample prediction: {y_pred_decode[:20]}')\n",
        "    print(classification_report(y_true_decode, y_pred_decode, digits=4))\n",
        "\n",
        "\n",
        "def define_train_evaluate_model(model, num_epoch=10):\n",
        "    # For recording the training log for visualisation\n",
        "    log = {\n",
        "        'train_loss': [],\n",
        "        'train_acc': [],\n",
        "        'train_f1': [],\n",
        "        'val_loss': [],\n",
        "        'val_acc': [],\n",
        "        'val_f1': [],\n",
        "    }\n",
        "\n",
        "    # Training Loop\n",
        "    for epoch in range(num_epoch):  \n",
        "        time1 = datetime.datetime.now()\n",
        "        train_loss = 0\n",
        "\n",
        "        model.train()\n",
        "        # Mini-Batch Training\n",
        "        for i, idxs in enumerate(train_input_index):\n",
        "            tags_index = train_output_index[i]\n",
        "\n",
        "            # Clear the gradients\n",
        "            model.zero_grad()\n",
        "\n",
        "            # Prepare the input and output data as tensors \n",
        "            sentence_in = torch.tensor(idxs, dtype=torch.long).to(device)\n",
        "            targets = torch.tensor(tags_index, dtype=torch.long).to(device)\n",
        "\n",
        "            # Forward propogation\n",
        "            loss = model.forward_training(sentence_in, targets)\n",
        "\n",
        "            # Backward Propogation\n",
        "            loss.backward()\n",
        "\n",
        "            # Update the model\n",
        "            optimizer.step()\n",
        "\n",
        "            # Accumulate the loss\n",
        "            train_loss+=loss.item()\n",
        "\n",
        "        # Evaluate the model on train_set and val_set\n",
        "        model.eval()\n",
        "        _, _, train_acc, train_f1 = cal_acc(model,train_input_index,train_output_index)\n",
        "        _, _, val_acc, val_f1 = cal_acc(model,val_input_index,val_output_index)\n",
        "\n",
        "        # Evaluate on the validation dataset \n",
        "        val_loss = 0\n",
        "        for i, idxs in enumerate(val_input_index):\n",
        "            tags_index = val_output_index[i]\n",
        "            sentence_in = torch.tensor(idxs, dtype=torch.long).to(device)\n",
        "            targets = torch.tensor(tags_index, dtype=torch.long).to(device)\n",
        "            loss = model.forward_training(sentence_in, targets)\n",
        "            val_loss+=loss.item()\n",
        "        time2 = datetime.datetime.now()\n",
        "\n",
        "        # The log of each epoch\n",
        "        print(\"Epoch:%d, Training loss: %.2f, train acc: %.4f, train f1: %.4f, val loss: %.2f, val acc: %.4f, val f1: %.4f, time: %.2fs\" \\\n",
        "            %(epoch+1, train_loss, train_acc, train_f1, val_loss, val_acc, val_f1, (time2-time1).total_seconds()))\n",
        "        # Log the training log of each epoch\n",
        "        log['train_loss'].append(train_loss)\n",
        "        log['train_acc'].append(train_acc)\n",
        "        log['train_f1'].append(val_acc)\n",
        "        log['val_loss'].append(val_loss)\n",
        "        log['val_acc'].append(val_acc)\n",
        "        log['val_f1'].append(val_acc)\n",
        "\n",
        "    # Evaluate the model\n",
        "    # evaluate_model(model)\n",
        "    return model, log\n",
        "# LSTM\n",
        "HIDDEN_DIM = 50                \n",
        "HIDDEN_LAYER = 1                \n",
        "\n",
        "# Attention\n",
        "ATTENTION_LAYER = 1             \n",
        "ATTENTION_TYPE = 'scaled_dot'   \n",
        "\n",
        "DROPOUT_RATE = 0.0\n",
        "\n",
        "# Training\n",
        "NUM_EPOCH = 2\n",
        "LR = 0.001\n",
        "WEIGHT_DECAY = 1e-4\n",
        "\n",
        "# Create the model \n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model_nocrf = Model(tag_to_ix, EMBEDDING_DIM, HIDDEN_DIM, HIDDEN_LAYER, DROPOUT_RATE, ATTENTION_LAYER, ATTENTION_TYPE).to(device)\n",
        "optimizer = optim.SGD(model_nocrf.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)\n",
        "print(f'Model Setup: \\n{model_nocrf}\\n')\n",
        "print(f'Optimizer Setup: \\n{optimizer}\\n')\n",
        "print(f'------ Start Training, Testing, Evaluating Process | Total Epoch: {NUM_EPOCH} ------')\n",
        "\n",
        "# Start Training, testing, evaluating process\n",
        "model_nocrf, log = define_train_evaluate_model(model_nocrf,\n",
        "                         num_epoch=NUM_EPOCH)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zW3gHeV9jMt7",
        "outputId": "c2d1e6a7-4ca9-49a7-f566-00aaafde31a4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Setup: \n",
            "Model(\n",
            "  (word_embeds): Embedding(11243, 25)\n",
            "  (lstm): LSTM(25, 25, bidirectional=True)\n",
            "  (attention_layers): ModuleList(\n",
            "    (0): Attention_layer(\n",
            "      (K): Linear(in_features=50, out_features=50, bias=False)\n",
            "      (Q): Linear(in_features=50, out_features=50, bias=False)\n",
            "      (V): Linear(in_features=50, out_features=50, bias=False)\n",
            "      (K_dropout): Dropout(p=0.0, inplace=False)\n",
            "      (Q_dropout): Dropout(p=0.0, inplace=False)\n",
            "      (V_dropout): Dropout(p=0.0, inplace=False)\n",
            "    )\n",
            "  )\n",
            "  (hidden2tag): Linear(in_features=50, out_features=12, bias=True)\n",
            "  (ce): CrossEntropyLoss()\n",
            ")\n",
            "\n",
            "Optimizer Setup: \n",
            "SGD (\n",
            "Parameter Group 0\n",
            "    dampening: 0\n",
            "    lr: 0.001\n",
            "    maximize: False\n",
            "    momentum: 0\n",
            "    nesterov: False\n",
            "    weight_decay: 0.0001\n",
            ")\n",
            "\n",
            "------ Start Training, Testing, Evaluating Process | Total Epoch: 2 ------\n",
            "Epoch:1, Training loss: 34726.24, train acc: 0.6750, train f1: 0.5440, val loss: 10215.26, val acc: 0.6772, val f1: 0.5469, time: 73.77s\n",
            "Epoch:2, Training loss: 29797.81, train acc: 0.6750, train f1: 0.5440, val loss: 9761.69, val acc: 0.6772, val f1: 0.5469, time: 73.94s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "0idMpYe-6Oqo"
      }
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "machine_shape": "hm",
      "name": "107A2",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}